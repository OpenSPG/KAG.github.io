风险内容位置：
    CommitId: 2911f9fb9d07e718b153121318ef12824a30a299
    文件名：website/docs/en/Examples/Publicdomaindatasets/benchmark.md
    文件行：1~57
风险内容：
    ---
    sidebar_position: 1
    ---
    
    # benchmark
    
    # Performance on multi-hop factual QA tasks
    The EM and F1 metrics of the version V0.6 of KAG, using the same experiment configuration as the KAG technical report ([https://arxiv.org/pdf/2409.13731](https://arxiv.org/pdf/2409.13731)), are as follows:
    
    | **** | **HotpotQA** | | **2Wiki** | | **MuSiQue** | **Average** |
    | --- | --- | --- | --- | --- | --- | --- |
    | | EM | F1 | EM | F1 | EM | F1 | EM | F1 |
    | GraphRAG | 0 |  | 0 |  | 0 |  | 0 |  |
    | lightRAG | 0 |  | 0 |  | 0 |  | 0 |  |
    | KAG TechReport<br/>（DeepSeek-V2 API） | 62.5 | 76.2 | 67.8 | 76.2 | 36.7 | 48.7 | 55.6 | 67.0 |
    | V0.6<br/>（DeepSeek-V2.5 API） | 60.9 | 75.4 | 69.6 | 78.6 | 36.1 | 48.2 | 55.5 | 67.4 |
    
    
    Note: The metrics of the KAG technical report are taken from Table 10 of the original text.
    
    # Performance on query-focused summarization (QFS) tasks
    + **Dataset**
    
    To evaluate the performance of KAG on query-focused summarization tasks, we compared the outputs of KAG and LightRAG on the [UltraDomain](https://huggingface.co/datasets/TommyChien/UltraDomain/tree/main) **cs.jsonl** dataset. Please refer to [KAG Example: CSQA](https://github.com/OpenSPG/KAG/tree/master/kag/examples/csqa).
    
    The **cs.jsonl** file contains 10 documents from the field of Computer Science, along with 100 questions and their corresponding answers based on these documents. Unlike the comparison method in the LightRAG paper, we used the questions provided in the **cs.jsonl** file rather than generating questions using a large language model for evaluation. Additionally, when calculating the factual correctness metric, we used the answers to the questions in the **cs.jsonl** file as the ground-truth.
    
    + **Quantitative evaluation results**
    
    | **** | **KAG** | **LightRAG** |
    | --- | --- | --- |
    | Comprehensiveness（0~10） | 7.57 | 8.87 |
    | Diversity（0~10） | 6.87 | 8.28 |
    | Empowerment（0~10） | 7.54 | 8.53 |
    | Factual Correctness（0~1） | 0.365 | 0.352 |
    | Construction time consumption | 4800 seconds | 3400 seconds |
    | Construction token consumption   | 7,006 K | 4,428 K |
    | Basic experiment configuration | generative model: deepseek-chat<br/>representational model: bge-m3<br/>concurrency:<br/>50 (num_threads_per_chain=50, num_chains=16)<br/>chunk size:<br/>(split_length=4950, window_length=100) | generative model: deepseek-chat<br/>representational model: bge-m3<br/>concurrency:<br/>50 (llm_model_max_async=50, embedding_func_max_async=50)<br/>chunk size:<br/>(chunk_token_size=1200, chunk_overlap_token_size=100) |
    
    
    + **Metric Interpretation**
    
    In this release, from the perspective of metrics, KAG has shown improvement in summarization tasks compared to the previous version, but there is still a gap compared to LightRAG. At the same time, in order to support both summarization generation and factual question answering, more work was done during knowledge extraction, which increased token consumption. We will continue to optimize this in future versions.
    
    EM and F1 metrics are not shown in the table because when we used the HotpotQA dataset for construction and evaluation, we found that the outputs of LightRAG, GraphRAG, and KAG (using the optimized prompt) all yielded an EM of 0 and an F1 close to 0 when compared to the HotpotQA evaluation dataset. We believe that EM and F1 are not suitable for evaluating the outputs of summarization tasks.
    
    
    
    We also acknowledge that the four evaluation metrics in the table are not perfect.
    
    + **Comprehensiveness**, **Diversity**, and **Empowerment** metrics are sensitive to the order of the answers during evaluation. Please refer to this [issue](https://github.com/HKUDS/LightRAG/issues/438) and the LightRAG paper.
    + **Factual Correctness** depends on large language models, and the output is unstable. We also experimented with the Factual Correctness calculation method from RAGAS, but it was even more unstable than the method shown in the CSQA example.
    
    If you have more reasonable evaluation methods, please feel free to provide feedback.
    
    
    
风险细节：
    敏感类型：URL地址(url)
    敏感细节：{hitWord=https://arxiv.org/pdf/2409.13731}
{hitWord=https://arxiv.org/pdf/2409.13731))}
{hitWord=https://huggingface.co/datasets/TommyChien/UltraDomain/tree/main)}
风险内容位置：
    CommitId: 2911f9fb9d07e718b153121318ef12824a30a299
    文件名：website/docs/en/FAQ.md
    文件行：1~122
风险内容：
    ---
    sidebar_position: 8
    ---
    
    # FAQ
    
    in the process of using, if you encounter any related questions about OpenSPG or KAG, you can ask questions through GitHub Issue or participate in Discussions at GitHub Discussions. Your questions will settle down and help other small partners.   
    Some problems may have been encountered before, you can search in the Issue. If you solve some problems in the process of using, you are also very welcome to interact in the community to solve and close the corresponding Issue.   
    
    
    | | **Issue **    | **Discussions **    |
    | --- | --- | --- |
    | **OpenSPG **    | [https://github.com/OpenSPG/openspg/issues](https://github.com/OpenSPG/openspg/issues)    | [https://github.com/OpenSPG/KAG/discussions](https://github.com/OpenSPG/KAG/discussions)    |
    | **KAG **    | [https://github.com/OpenSPG/KAG/issues](https://github.com/OpenSPG/KAG/issues)    | [https://github.com/OpenSPG/openspg/discussions](https://github.com/OpenSPG/openspg/discussions)    |
    
    
    ## question 1: figure storage configuration, model configuration, whether the vector configuration has built-in services 
    the graph storage configuration and vector configuration have corresponding built-in services after the image is installed through the docker-compose.yml. For scenarios with low performance requirements and quick verification, you can directly use the following configurations
    
    **Graph storage configuration **
    
    ```json
    {
      "uri":"neo4j://release-openspg-neo4j:7687",
      "user":"neo4j"
    }
    ```
    
    the model service is not built in yet. You need to build the model Service or call a third-party API. For details, see: [Generate (chat) model configuration](https://openspg.yuque.com/ndx6g9/cwh47i/tx0gd5759hg4xi56)  [Representation (embedding) model](https://openspg.yuque.com/ndx6g9/cwh47i/nmq2aq4s11b6mgxx)
    
    ## Question 2: Why does the project configuration file kag_config.cfg not take effect after modification? 
    Solution: Because the relevant metadata is stored in the server (openspg-server), you need to update the corresponding configuration to the server after modifying the current KAG file. Step: In the project Directory, run the next project update -- proj_path. To update the project configuration, and directly modify the corresponding configuration information in the OpenSPG server page. For other commands, please refer: [Command Line Tools](https://openspg.yuque.com/ndx6g9/cwh47i/rrcdpyq9gv13fcc8)
    
    ## Question 3: I want to keep or empty the data in the Neo4j image, how should I set it? 
    If you need to configure the data on the host computer- $HOME/dozerdb/data:/data in the docker-compose.yml, remove the code and delete it anyway. Container when the data will also be lost, the default is not set. Same with MySQL.
    
    ![1730275758456-fb48f241-59c7-4973-b058-c23f6d9876bf.png](./img/Rsd3nPFZDV5J4onl/1730275758456-fb48f241-59c7-4973-b058-c23f6d9876bf-115618.png)
    
    
    
    ## Question 4: After the generation model and the representation model are correctly configured and can be successfully accessed through the curl Command on the host machine, connection refused or timeout still appears in the knowledge question and answer page of the OpenSPG server. 
    The OpenSPG Q &amp; A page conducts natural language Q &amp; A. The container that needs to be spg-server can normally access the vector service on the host machine. For students using docker desktop on windows or mac, you can specify the vector service base_url = http:/host.docker.int ernal:11434/${path} to access the host machine. For students using linux, you can specify base_url = [http://172.17.0.1:11434/${path}](http://172.17.0.1:11434/${path}) access the host by accessing the gateway of the docker0 network. In addition, before starting the ollama service, export OLLAMA_HOST = 0.0.0.0:11434, configure ollama to listen to access requests from all addresses.  
    At the same time, when we create a project, we will verify all the configurations and save them only after they pass the verification. 
    
    
    
    ## Question 5: My document is relatively large. I found that there are too many tokens consumed when calling the model service. How should I solve it? 
    When using the product, the length of Chunk segmentation can be appropriately adjusted according to the actual situation, for example, from 200 to 2000 or even higher. The smaller the segmentation length, the better the subsequent extraction effect is, and can be adjusted according to the actual situation. 
    
    ## Question 6: Why can't I find the corresponding Python Package? No matching distribution found for openspg-knext = = xxxx 
    the openspg package is deployed in pypi.org. You can check whether the corresponding package exists at the following address:   
    [https://pypi.org/project/openspg-kag](https://pypi.org/project/openspg-kag)  
    [https://pypi.org/project/openspg-knext](https://pypi.org/project/openspg-knext)  
    if the version that reported the error already exists, please check whether the local environment has been proxied or forwarded to other pypi warehouses.   
    If the wrong version is not found in pypi.org, please contact us in time.
    
    ## Question 7: Is the front-end code open source kag-model when 
    **front End Code **: At present, there is no open source plan for the front-end code. The future will judge whether it is open source or not after evaluating the community usage.  
    **kag-model **: KAG's subsequent work priorities for kag-model optimization. We will open source in subsequent versions. KAG's subsequent plans are as follows: 
    
    + domain knowledge injection to realize the fusion of Domain concept graph and entity graph 
    + kag-model optimization to improve the efficiency of image composition and question answering
    + hallucination Inhibition of Knowledge Logic Constraints
    
    ## Question 8: How to view the contents of a checkpoint file 
    The KAG framework supports two types of checkpoint formats: text and binary. The text format is used for recording execution logs and is located at the top level of the ckpt directory. The binary format is used for recording intermediate results from various stages of task execution, such as the graph data extracted by the Extractor component. 
    
    + **Viewing Statistics **
    
    ![1736158479602-67569500-1d87-4f52-9e03-133d8fb76f41.png](./img/Rsd3nPFZDV5J4onl/1736158479602-67569500-1d87-4f52-9e03-133d8fb76f41-790829.png)
    
    
    
    ```bash
    less kag_checkpoint_0_1.ckpt
    ```
    
    It records the number of nodes, edges, and graphs constructed from each record. 
    
    ![1736158513683-f4f51f5b-8e32-438d-badc-eab5ddc09424.png](./img/Rsd3nPFZDV5J4onl/1736158513683-f4f51f5b-8e32-438d-badc-eab5ddc09424-206795.png)
    
    + **Viewing ****binary**
    
    The binary format of the checkpoint is based on ZODB for key-value storage. Users can refer to the official ZODB documentation for more detailed usage information: [https://zodb.org/en/latest/](https://zodb.org/en/latest/)
    
    Here is a sample program that extracts all the raw graph data from the checkpoint file of the extractor component and writes it into a JSONL file for further analysis and optimization:   
    
    
    ```bash
    import json
    import pickle
    from ZODB import DB
    from ZODB.FileStorage import FileStorage
    
    
    storage = FileStorage("extractor/kag_checkpoint_0_1.ckpt")
    db = DB(storage)
    connection = db.open()
    print(len(connection.root.data))
    graphs = []
    for k, v in connection.root.data.items():
        """
        The data is pickled to prevent it from being modified by ZODB in subsequent processes. 
        So it needs to be deserialized upon reading.
        """
        graphs+=pickle.loads(v)
    with open("extracted_subgraphs.jsonl", "w") as writer:
        for g in graphs:
            writer.write(json.dumps(g.to_dict(), ensure_ascii=False))
            writer.write("\n")
    ```
    
    It's important to note that ZODB can track modifications to objects within the program and automatically sync these changes to the file. Therefore, we use pickle to serialize the data into an immutable byte sequence before writing it to ZODB, to prevent any modifications by ZODB in subsequent processing components. When reading the data, we need to use pickle to deserialize it and retrieve the original data. 
    
    ## Question 9 How to custom KagBuilder & KagSolver Task
    The KAG framework introduced a registry-based code and configuration management mechanism in version 0.6. Users can override the built-in components of KAG and register their custom implementations to replace the default ones. For more details, please refer to the [KagBuiler & KagSolver customization](https://openspg.yuque.com/ndx6g9/cwh47i/ui1vgeez17zuqxsa)
    
      
      
      
      
    
风险细节：
    敏感类型：URL地址(url)
    敏感细节：{hitWord=https://openspg.yuque.com/ndx6g9/cwh47i/tx0gd5759hg4xi56)}
{hitWord=https://openspg.yuque.com/ndx6g9/cwh47i/nmq2aq4s11b6mgxx)}
{hitWord=https://openspg.yuque.com/ndx6g9/cwh47i/rrcdpyq9gv13fcc8)}
{hitWord=https://pypi.org/project/openspg-kag}
{hitWord=https://pypi.org/project/openspg-kag)}
{hitWord=https://pypi.org/project/openspg-knext}
{hitWord=https://pypi.org/project/openspg-knext)}
{hitWord=https://zodb.org/en/latest/}
{hitWord=https://zodb.org/en/latest/)}
{hitWord=https://openspg.yuque.com/ndx6g9/cwh47i/ui1vgeez17zuqxsa)}
风险内容位置：
    CommitId: 2911f9fb9d07e718b153121318ef12824a30a299
    文件名：website/docs/en/UserGuide/CommandLineTools.md
    文件行：1~266
风险内容：
    ---
    sidebar_position: 9
    ---
    
    # Command Line Tools
    
    # 1、Command Line Tools
    Using the knext command-line tool and its various subcommands, you can achieve a complete workflow for building and using graph data.   
    
    
    ```bash
    # show the user maual
    
    $ knext --help
    Usage: knext [OPTIONS] COMMAND [ARGS]...
    
    Options:
      --version  Show the version and exit.
      --help     Show this message and exit.
    
    Commands:
      project   Project client.
      reasoner  Reasoner client.
      schema    Schema client.
      thinker   Thinker client.
    ```
    
    ## 1.1、project
    ```bash
    # show the user maual
    $ knext project --help
    Usage: knext project [OPTIONS] COMMAND [ARGS]...
    
      Project client.
    
    Options:
      --help  Show this message and exit.
    
    Commands:
      create   Create new project with a demo case.
      list
      restore
      update
    ```
    
    ### 1.1.1、project create
    ```bash
    # show the user maual
    $ knext project create --help
    
    Usage: knext project create [OPTIONS]
    
      Create new project with a demo case.
    
    Options:
      --config_path TEXT        Path of config.  [required]
      --tmpl [default|medical]  Template of project, use default if not specified.
      --help                    Show this message and exit.
    ```
    
    + 【required】--config_path，Path of config
    
    ```bash
    # show the user maual
    $ knext project create --config_path kag_config.yaml
    ```
    
    After successful execution, a directory named KagDemo will be created in the current directory. Navigate into the example project by executing cd KagDemo.
    
    ```bash
    KagDemo
    ├── builder
    │   ├── __init__.py
    │   ├── data
    │   ├── indexer.py
    │   └── prompt
    ├── kag_config.yaml
    ├── reasoner
    │   └── __init__.py
    ├── schema
    │   ├── KagDemo.schema
    │   └── __init__.py
    └── solver
        ├── __init__.py
        └── prompt
    ```
    
    ### 1.1.2、project restore
    User can use this command to quickly restore a project from an existing project directory. 
    
    ```bash
    # show the user maual
    $ knext project restore --help
    
    Usage: knext project restore [OPTIONS]
    
    Options:
      --host_addr TEXT  Address of spg server.
      --proj_path TEXT  Path of config.
      --help            Show this message and exit.
    ```
    
    + --proj_path  Path of config.
    + --host_addr Address of spg server.
    
    ```bash
    $ knext project restore --proj_path KagDemo --host_addr http://127.0.0.1:8887
    ```
    
    In the YAML file, a new id will be added under [project]. If the project already exists on the server, it will return the project's ID. If the project does not exist on the server, it will create a new project and return its ID.
    
    ### 1.1.3、project update
    When the kag_config.yaml file is updated, you need to execute this command to sync the configuration to the SPG Server. 
    
    ```bash
    # show the user maual
    $ knext project update --help
    
    Usage: knext project update [OPTIONS]
    
    Options:
      --proj_path TEXT  Path of config.
      --help            Show this message and exit.
    ```
    
    + --proj_path，Path of config
    
    ```bash
    knext project update --proj_path KagDemo
    ```
    
    ### 1.1.4、project list
    ```bash
    # show the user manual
    $ knext project list --host_addr http://127.0.0.1:8887
    
    Project Name | Project ID
    -------------------------
    RiskMining           | 1         
    SupplyChain          | 2         
    TwoWiki              | 3         
    Medicine             | 4         
    EventDemo            | 5         
    HotpotQA             | 6 
    ```
    
    --host_addr: The address of the SPG server, defaulting to http://127.0.0.1:8887. 
    
    ## 1.2、schema
    ```bash
    Usage: knext schema [OPTIONS] COMMAND [ARGS]...
    
      Schema client.
    
    Options:
      --help  Show this message and exit.
    
    Commands:
      commit            Commit local schema and generate schema helper.
      reg_concept_rule  Register a concept rule according to DSL file.
    ```
    
    ### 1.2.1 Commit schema
    When a project is created or restored, or when the schema file is updated, this command needs to be executed to synchronize the schema to the SPG Server.
    
    ```bash
    # show the user maual
    $ knext schema commit --help
    
    Usage: knext schema commit [OPTIONS]
    
      Commit local schema and generate schema helper.
    
    Options:
      --help  Show this message and exit.
    ```
    
    ### 1.2.2 Register concept rules
    ```bash
    # show the user maual
    $ knext schema reg_concept_rule --help
    
    Usage: knext schema reg_concept_rule [OPTIONS]
    
      Register a concept rule according to DSL file.
    
    Options:
      --file TEXT  Path of DSL file.
      --help       Show this message and exit.
    ```
    
    + [Required] --file Path of the concept rule file
    
    Example:
    
    schema/concept.rule
    
    ```plain
    namespace DEFAULT
    
    `TaxOfRiskApp`/`赌博应用`:
        rule: [[
            ...
        ]]
    ```
    
    Command:
    
    ```bash
    $ knext schema reg_concept_rule --file schema/concept.rule
    ```
    
    Result:
    
    ```plain
    Defined belongTo rule for ...
    ...
    Concept rule is successfully registered.
    ```
    
    ### 1.3 reasoner
    ```bash
    $ knext reasoner --help
    
    Usage: knext reasoner [OPTIONS] COMMAND [ARGS]...
    
      Reasoner client.
    
    Options:
      --help  Show this message and exit.
    
    Commands:
      execute   Query dsl by providing a string or file.
    ```
    
    ### 1.3.1 KGDSL query
    Submit a KGDSL query job and generate result synchronously.  
    If the query job takes more than 3 minutes, an exception will be thrown.
    
    ```bash
    $ knext reasoner execute [--file] [--dsl]
    ```
    
    + [Optional] --file The KGDSL file to query.
    + [Optional] --dsl The KGDSL syntax to query, enclosed in double quotation marks.
    
    Example:
    
    ```bash
    MATCH (s:Demo.Company)
    RETURN s.id, s.address
    ```
    
    ```bash
    $ knext reasoner execute --file reasoner/demo.dsl
    ```
    
    Result：
    
    ```bash
    |   s_id | s_demoProperty   |
    |--------|------------------|
    |     00 | demo             |
    ```
    
    
风险细节：
    敏感类型：URL地址(url)
    敏感细节：{hitWord=http://127.0.0.1:8887.}
风险内容位置：
    CommitId: 2911f9fb9d07e718b153121318ef12824a30a299
    文件名：website/docs/en/UserGuide/Customextensions/MountingDomainKnowledge.md
    文件行：1~251
风险内容：
    ---
    sidebar_position: 4
    ---
    
    # Mounting Domain Knowledge
    
    # Introduction
    In certain vertical domains such as healthcare, law, and finance, a substantial amount of structured expert knowledge graphs may have been accumulated. These domain-specific knowledge graphs can serve as a priori for unstructured knowledge graph extraction. They not only aid in knowledge extraction but also effectively enhance the overall quality of the knowledge graph, thereby improving the accuracy of reasoning and question-answering.
    
    To support the integration of domain knowledge graphs, KAG introduced the ExternalGraphLoader component in version 0.6, offering the following fundamental features:
    
    1. Loading and importing existing domain knowledge graphs into the graph database.
    2. Utilizing domain knowledge graph nodes for entity recognition in documents within the Extractor component.
    3. In the PostProcessor component, linking and standardizing entities extracted from unstructured documents to nodes in the domain knowledge graph.
    
    Through these methods, users can integrate graphs constructed from unstructured documents with existing domain knowledge graphs, resulting in a higher quality index.
    
    
    
    # Example
    An example of medical domain knowledge mounting(injection) is provided in KAG, where the nodes of the domain knowledge graph are medical terms, and the relationships are defined as isA. The document content includes introductions to various medical terms. You can refer to the readme document for an initial experience. 
    
    **Code Path: **`**kag/examples/domain_kg/**`
    
    # Implementation of the Domain Knowledge Mounting Component 
    The base class of domain knowledge mounting component  is`kag.interface.builder.external_graph_abc.ExternalGraphLoaderABC`，Users can use the following command to view detailed information of the default implementation:
    
    ```bash
    $ kag interface --cls ExternalGraphLoaderABC
    ```
    
    The aforementioned command will print the following information:
    
    ```plain
                        Documentation of ExternalGraphLoaderABC
    Abstract base class for loading and interacting with external knowledge graphs.
    
    This class defines the interface for components that load and interact with external knowledge graphs.
    It inherits from `BuilderComponent` and provides methods for dumping subgraphs, performing named entity
    recognition (NER), retrieving allowed labels, and matching entities.
                        Registered subclasses of ExternalGraphLoaderABC
    [kag.builder.component.external_graph.external_graph.DefaultExternalGraphLoader]
    Register Name: "base"
    
    Documentation:
    A default implementation of the ExternalGraphLoaderABC interface.
    
    This class is responsible for loading external graph data based on the provided nodes, edges, and match configuration.
    
    Initializer:
    Creates an instance of DefaultExternalGraphLoader from JSON files containing node and edge data.
    
    Args:
        node_file_path (str): The path to the JSON file containing node data.
        edge_file_path (str): The path to the JSON file containing edge data.
        match_config (MatchConfig): The configuration for matching query str to graph nodes.
    
    Returns:
        DefaultExternalGraphLoader: An instance of DefaultExternalGraphLoader initialized with the data from the JSON files.
    
    Required Arguments:
      node_file_path: str
      edge_file_path: str
      match_config: MatchConfig
    
    Optional Arguments:
      No Optional Arguments found
    
    Sample Useage:
      ExternalGraphLoaderABC.from_config({'type': 'base', 'node_file_path': 'Your node_file_path config', 'edge_file_path': 'Your edge_file_path config', 'match_config': 'Your match_config config'})
    
    ```
    
    It can be seen that KAG currently offers a default implementation registered under the name "`base`", with the class path being `kag.builder.component.external_graph.external_graph.DefaultExternalGraphLoader`. 
    
    
    
    Below is a detailed explanation of the key interfaces.
    
    ## Component Initialization
    The initialization interface of the `DefaultExternalGraphLoader` contains the following parameters:
    
    
    
    + `node_file_path (str)`
    
    The node data of the domain knowledge graph is in JSON format, resembling: []. 
    
    ```json
    [
        {
            "id": "00000001,
            "name": "(缩)肾上腺皮质激素",
            "label": "Concept",
            "properties": {
    
            }
        },
        {
            "id": "00000002",
            "name": "促肾上腺皮质激素",
            "label": "Concept",
            "properties": {
    
            }
        }
    ]
    ```
    
    
    
    This format corresponds to the default graph node data model in the KAG framework (`kag.builder.model.sub_graph.Node`). The fields are described as follows:
    
        -  `id`: The unique identifier of the node.
        -  `name`: The name of the node.
        -  `label`: The type of the node, which must exist within the schema definition.
        -  `properties`: An optional field for storing additional attributes of the node.
    
    
    
    + `edge_file_path (str)`
    
    The relationship data of the domain knowledge graph is in JSON format, resembling: 
    
    
    
    ```json
    [
        {
            "id": "00000001-00000002",
            "from": "00000001",
            "fromType": "Concept",
            "to": "00000002",
            "toType": "Concept",
            "label": "isA",
            "properties": {
    
            }
        },
        {
            "id": "00000001-00000003",
            "from": "00000001",
            "fromType": "Concept",
            "to": "00000003",
            "toType": "Concept",
            "label": "isA",
            "properties": {
    
            }
        }
    ]
    ```
    
    This format corresponds to the default graph relationship data model in the KAG framework (`kag.builder.model.sub_graph.Edge`). The fields are described as follows:
    
        - `id`: The unique identifier of the relationship.
        - `label`: The type of the relationship.
        - `from`: The starting point ID of the relationship.
        - `fromType`: The type of the starting point of the relationship.
        - `to`: The ending point ID of the relationship.
        - `toType`: The type of the ending point of the relationship.
        - `properties`: An optional field for storing additional attributes of the relationship.
    
    
    
    + `match_config (MatchConfig)`
    
    An object of type `kag.interface.MatchConfi`g, which defines the configuration for linking external entities to entity nodes within the domain knowledge graph. Its implementation is as follows:
    
    ```python
    class MatchConfig(Registrable):
        """
        Configuration class for matching operations.
    
        This class is used to define the parameters for matching operations, such as the number of matches to return,
        the labels to consider, and the threshold for matching confidence.
    
        Attributes:
            k (int): The number of matches to return. Defaults to 1.
            labels (List[str]): The list of labels to consider for matching. Defaults to None.
            threshold (float): The confidence threshold for matching. Defaults to 0.9.
        """
    
        def __init__(self, k: int = 1, labels: List[str] = None, threshold: float = 0.9):
            """
            Initializes the MatchConfig with the specified parameters.
    
            Args:
                k (int, optional): The number of matches to return. Defaults to 1.
                labels (List[str], optional): The list of labels to consider for matching. Defaults to None.
                threshold (float, optional): The confidence threshold for matching. Defaults to 0.9.
            """
            self.k = k
            self.labels = labels
            self.threshold = threshold
    
    ```
    
    ## Performing Entity Recognition Based on Domain Knowledge Graphs
    The `DefaultExternalGraphLoader` offers an NER (Named Entity Recognition) interface that enables the identification of entities within text by leveraging the entities contained in the domain knowledge graph. Compared to entity recognition based on LLM (Large Language Models), this approach allows for a more effective integration of domain-specific knowledge. The interface is defined as follows:
    
    ```python
        def ner(self, content: str):
            output = []
            import jieba
    
            for word in jieba.cut(content):
                if word in self.vocabulary:
                    output.append(self.vocabulary[word])
            return output
    
    ```
    
    The current implementation relies on Chinese word segmentation and string matching, and users are encouraged to extend it with additional strategies as needed.
    
    
    
    ## Entity Linking Based on Domain Knowledge Graphs
    In numerous scenarios, there is a need to link extracted entities to standardized domain knowledge entity nodes, effectively normalizing the entities. For open domains, this task can be adeptly accomplished with the assistance of LLMs (Large Language Models). However, within specific vertical domains, LLMs often lack the requisite knowledge or concepts. The `DefaultExternalGraphLoader` provides a match_entity interface, which facilitates the search for the most similar node in the domain knowledge graph for any given entity through text matching or vector matching. The interface is defined as follows:
    
    ```python
        def match_entity(self, query: Union[str, List[float], np.ndarray]):
            if isinstance(query, str):
                return self.text_match(
                    query, k=self.match_config.k, labels=self.match_config.labels
                )
            else:
                return self.vector_match(
                    query,
                    k=self.match_config.k,
                    labels=self.match_config.labels,
                    threshold=self.match_config.threshold,
                )
    
    ```
    
    Herein, `match_config` delineates the strategy for entity linking, such as the number of entities to link and the similarity threshold, among other parameters.
    
    
    
    # Custom extensions
    The domain knowledge integration components provided herein are equipped with core functionalities that may not comprehensively address all use cases. Users seeking to fulfill specific requirements are encouraged to consult the[ custom code section](https://openspg.yuque.com/ndx6g9/docs_en/ui1vgeez17zuqxsa) for guidance on extending these components. Key areas for extension encompass:
    
    1. Extending the `ExternalGraphLoader` Component  
    For instance, enabling support for loading domain knowledge files in various formats, and customizing ner and match_entity strategies.
    2. Extending the `Extractor` Component  
    For example, customizing how domain knowledge intervenes or corrects extraction results during the extraction process.
    3. Extending the `Solver` Component  
    Such as prioritizing retrieval and recall based on domain knowledge during the reasoning and question-answering process.
    
       
风险细节：
    敏感类型：URL地址(url)
    敏感细节：{hitWord=https://openspg.yuque.com/ndx6g9/docs_en/ui1vgeez17zuqxsa)}
风险内容位置：
    CommitId: 2911f9fb9d07e718b153121318ef12824a30a299
    文件名：website/docs/en/QuickStart.md
    文件行：1~667
风险内容：
    ---
    sidebar_position: 1
    ---
    
    # Quick Start
    
    # 1 Pre-Requirements 
    ## 1.1 Hardware Requirements
    ```bash
    CPU ≥ 8 cores;
    RAM ≥ 32 GB;
    Disk ≥ 100 GB;
    ```
    
    ## 1.2 Software Requirements 
    + **operating System **
    
    ```bash
    macOS System：macOS Monterey 12.6 or later
    Linux System：CentOS 7 / Ubuntu 20.04 or later
    Windows System：Windows 10 LTSC 2021 or later
    ```
    
    + **application software **
    
    ```bash
    macOS / Linux System：Docker，Docker Compose
    Windows System：WSL 2 / Hyper-V，Docker，Docker Compose
    
    Docker ≥ 24.0.0 & Docker Compose ≥ v2.26.1.
    ```
    
    ## 1.3 Generate model
    KAG supports all generative model services with OpenAI-compatible class interfaces, such as DeepSeek, Qwen, OpenAI, etc.
    
    Developers can go to the official websites of large commercial models such as [deepseek official website](https://www.deepseek.com/), [tongyi official website](https://tongyi.aliyun.com/), [openai official website](https://chat.openai.com/) to complete account registration and activation of model services in advance, obtain the api-key, and fill it in the subsequent project configuration.
    
    KAG also supports docking with the generative model prediction services provided by Ollama, Xinference, etc. For details, please refer to the relevant chapters on model services. **In the quick start stage, it is strongly recommended to purchase a commercial large model API to complete the trial run verification.**
    
    ## **1.4 Representation model **
    KAG supports representation model services with OpenAI-compatible class interfaces, such as OpenAI, Silicon Mobility, etc.
    
    Developers can go to the official website of [Silicon flow website](https://docs.siliconflow.cn/api-reference/embeddings/create-embeddings), [openai official website](https://chat.openai.com/) and other commercial model websites to complete account registration and activation of model services in advance, obtain the API key, and fill it in the subsequent project configuration.
    
    KAG also supports docking with representation model prediction services provided by Ollama, Xinference, etc. For details, please refer to the relevant chapters on model services. **In the quick start stage, it is strongly recommended to purchase a commercial large model API to complete the trial run verification.**
    
    # 2 **Service deployment**
    ## 2.1 Start the service
    ```bash
    # set HOME enviroment var（only for Windows Users）
    # set HOME=%USERPROFILE%
    
    # get docker-compose.yaml file
    $ curl -sSL https://raw.githubusercontent.com/OpenSPG/openspg/refs/heads/master/dev/release/docker-compose-west.yml -o docker-compose-west.yml
    # service start
    $ docker compose -f docker-compose-west.yml up -d
    ```
    
    Tip: If you want to persist data in a local directory, you can do this through Docker's volumes configuration. This way, the data will not be lost even if the container is restarted or deleted. Add the following content to the volumes of mysql and neo4j in docker-compose.yml
    
    ![1733968199612-c49d1423-1b80-401d-9779-591d615db43d.png](./img/LPB7FOFlrQHK4jpL/1733968199612-c49d1423-1b80-401d-9779-591d615db43d-993255.png)
    
    ![1733968199612-c49d1423-1b80-401d-9779-591d615db43d.png](./img/LPB7FOFlrQHK4jpL/1733968199612-c49d1423-1b80-401d-9779-591d615db43d-993255.png)
    
    ## 2.2 **View Status**
    ```yaml
    $ docker ps
    ```
    
    ![1736213735701-9db33549-dd59-4c2f-9832-763fb5b582b8.png](./img/LPB7FOFlrQHK4jpL/1736213735701-9db33549-dd59-4c2f-9832-763fb5b582b8-380535.png)
    
    ```yaml
    $ docker logs -f release-openspg-server
    ```
    
    The following log shows the successful start of the openspg server
    
    ![1733968319690-496bc1e0-37a2-4810-9bd5-82dda381caf5.png](./img/LPB7FOFlrQHK4jpL/1733968319690-496bc1e0-37a2-4810-9bd5-82dda381caf5-087969.png)
    
    ## 2.3 **Product Access**
    Enter [http://127.0.0.1:8887](http://127.0.0.1:8887) in the browser to access the OpenSPG-KAG product interface.
    
    ```sql
    # Default login information:
    # Default Username: openspg
    # Default password: openspg@kag
    
    # The default password must be changed before it can be used. 
    # If you forget the password, you can reinitialize it in the database with the following command:
    UPDATE kg_user SET `gmt_create` = now(),`gmt_modified` = now(),`dw_access_key` ='efea9c06f9a581fe392bab2ee9a0508b2878f958c1f422f8080999e7dc024b83' where user_no = 'openspg' limit 1;
    ```
    
    ![1736238979301-4c4e660e-04a3-43d1-a3e2-90f98e75a1de.png](./img/LPB7FOFlrQHK4jpL/1736238979301-4c4e660e-04a3-43d1-a3e2-90f98e75a1de-917081.png)
    
    # 3 **KAG usage (product mode) **
    This product provides a visual interface for OpenSPG-KAG, supporting users to build, ask and manage private domain knowledge bases on the page. At the same time, the modeling results can be viewed intuitively.  
    When used in production mode, the KAG package python package is built into the openspg container, providing default building and reasoning question-answering capabilities. The calls to the generation model and representation model in the knowledge extraction and graph reasoning stages are all initiated from the openspg server container environment.
    
    [快速开始](https://openspg.yuque.com/ndx6g9/0.5/nbb1bn3wegwue6yo?inner=zb7FU)
    
    ## 3.1 **Create a knowledge base**
    ### 3.1.1 Global configuration
    ![1736239018279-05d4d0df-ba25-45a4-85f8-d5ac2ae537bc.png](./img/LPB7FOFlrQHK4jpL/1736239018279-05d4d0df-ba25-45a4-85f8-d5ac2ae537bc-293925.png)
    
    + **Common Configuration**
    
    user can custom database & vector model & prompt in common configuration
    
    ![1736239032870-0a55b6b9-30e1-4367-be10-403ce9fbd9c0.png](./img/LPB7FOFlrQHK4jpL/1736239032870-0a55b6b9-30e1-4367-be10-403ce9fbd9c0-042308.png)
    
        - **database configuration **
    
    By default, the local openspg-neo4j graph storage database can be filled in. Example:
    
    ```json
    {
      "database":"neo4j", # default datahbase name, which will be replaced by namespace of knowledge base
      "uri":"neo4j://release-openspg-neo4j:7687", # neo4j server address, which can be replaced by customized neo4j server which is accessbile
      "user":"neo4j", # neo4j username, default to neo4j
      "password":"neo4j@openspg", # neo4j password, default to neo4j@openspg
    }
    ```
    
        - **vector configuration**
    
    For details, refer to the [Representation (embedding) model](https://openspg.yuque.com/ndx6g9/docs_en/nmq2aq4s11b6mgxx). Example of configuring a commercial  embedding model service such as siliconflow:
    
    ```json
    {
      "type":"openai", # KAG supports openai compatible interface of embedding service
      "model":"BAAI/bge-m3", # model name of embedding service
      "base_url":"https://api.siliconflow.cn/v1", # url of embedding service
      "api_key":"your api key"
    }
    ```
    
        - **prompt**
    
    Required. Used to determine whether to use Chinese (zh) or English (en) when calling the model. Example:
    
    ```json
    {
      "biz_scene":"default", # biz_scene for kag template
      "language":"en", # en for english and zh for chinese
    }
    ```
    
    + **Model configuration**
    
    KAG Supports Open-AI compatible generative Model APIs (chatgpt, deepseek, qwen2, etc.)，provides maas, vllm, ollama and other modes, for details, refer to [Generate (chat) model configuration](https://openspg.yuque.com/ndx6g9/docs_en/tx0gd5759hg4xi56). 
    
    ![1736239051970-2d34b883-efa1-4121-be06-bd49169b8465.png](./img/LPB7FOFlrQHK4jpL/1736239051970-2d34b883-efa1-4121-be06-bd49169b8465-947460.png)
    
        - **maas**
    
    ```json
    {
      "model": "deepseek-chat",
      "base_url": "https://api.deepseek.com",
      "api_key": "deepseek api key"
    }
    ```
    
    + **User Configuration**
    
    Account management can be done in User Configuration, including create/delete user, change password, etc.
    
    ![1736239064481-ec566aad-19e7-402e-8f6d-7aba34f19c7e.png](./img/LPB7FOFlrQHK4jpL/1736239064481-ec566aad-19e7-402e-8f6d-7aba34f19c7e-688291.png)
    
    ### 3.1.2 **New Knowledge Base **
    User can use global configuration for specific knowledge base, or customize a new configuration.
    
    ![1736239082492-50045f96-445a-40b7-bbd6-de7bb05377e3.png](./img/LPB7FOFlrQHK4jpL/1736239082492-50045f96-445a-40b7-bbd6-de7bb05377e3-749603.png)
    
    + **namespace and graphStore configuration**
    
    we can use default configuration which was settled in global conf, default database name would be replaced by name of knowledge base.
    
    ![1736239105912-e1b4865b-f5ba-4fb2-8a18-94edb0ce39ee.png](./img/LPB7FOFlrQHK4jpL/1736239105912-e1b4865b-f5ba-4fb2-8a18-94edb0ce39ee-948420.png)
    
    + **vector configuration & prompts **
    
    we can use default configuration which was settled in global conf.
    
    **Special attention: **embedding vectors generated by different representation models cannot be mixed even if they have the same dimensions; Therefore, in the knowledge base configuration, the configuration related to the representation model cannot be modified once it is set. 
    
    ![1736239133942-3d8e7a90-2945-4de1-b11a-bb0aa0ef4de0.png](./img/LPB7FOFlrQHK4jpL/1736239133942-3d8e7a90-2945-4de1-b11a-bb0aa0ef4de0-616320.png)
    
    ## 3.2 **Import documents **
    ### 3.2.1 Create a build task
    Enter the knowledge base Build => Create task to initiate knowledge building tasks. Users can download sample files  [David Eagleman.txt](https://openspg.yuque.com/attachments/yuque/0/2025/txt/32480677/1735889862217-c48e42ce-696d-45d5-8903-08e84bf5f841.txt) [Karl Deisseroth.txt](https://openspg.yuque.com/attachments/yuque/0/2025/txt/32480677/1735889862633-c2cc2584-b4d7-4990-8398-3a73dd95a8a3.txt) [Thomas C. Sudhof.txt](https://openspg.yuque.com/attachments/yuque/0/2025/txt/32480677/1735889862788-86c08705-ca8a-4f01-bb69-f87d60c4f736.txt) for multi-hop Q&A tasks testing.
    
    ![1734592705789-d2744c35-ec94-490f-bd2c-044143bc1934.gif](./img/LPB7FOFlrQHK4jpL/1734592705789-d2744c35-ec94-490f-bd2c-044143bc1934-273516.gif)
    
    ### 3.2.2 **Check knowledge extraction Results**
    Users can view the graph data by clicking on the [Knowledge Exploration] menu on the product side.
    
    Users can refer to [Knowledge Exploration](https://openspg.yuque.com/ndx6g9/cwh47i/mzq74eaynm4rqx4b) doc for detail.
    
    ![1736224942237-ae839ce6-9537-4062-a96a-208d6b189e98.png](./img/LPB7FOFlrQHK4jpL/1736224942237-ae839ce6-9537-4062-a96a-208d6b189e98-528085.png)
    
    
    
    ![1736224895758-e147c7fc-e3b6-449c-8a09-e39f85171e56.png](./img/LPB7FOFlrQHK4jpL/1736224895758-e147c7fc-e3b6-449c-8a09-e39f85171e56-535048.png)
    
    ## 3.3 **Reasoning Questions and Answers **
    Enter the question `Which Stanford University professor works on Alzheimer's?` and wait for the result to return. 
    
    ![1736229014430-5fa7d891-0fb7-4fb5-80da-2642b8d2207f.png](./img/LPB7FOFlrQHK4jpL/1736229014430-5fa7d891-0fb7-4fb5-80da-2642b8d2207f-933421.png)
    
    # 4 **KAG usage (developer mode)**
    In the context of private domain knowledge bases, the effectiveness of graph construction and reasoning-based question answering is closely tied to schema design, knowledge extraction prompts, the selection of representation models, question planning prompts, graph retrieval algorithms, and answer generation prompts. These customizations are not yet exposed on the product side, requiring users to leverage the KAG developer mode to implement their customizations.
    
    When using the developer mode, users execute the KAG Python package code in their local environment. The OpenSPG server solely provides capabilities such as schema management, reasoning execution, and graph database adaptation. Calls to generative models and representation models during the knowledge extraction and graph reasoning phases are initiated from the local environment.
    
    [快速开始](https://openspg.yuque.com/ndx6g9/0.5/nbb1bn3wegwue6yo?inner=S8PoP)
    
    ## 4.0、Video Tutorial
    [此处为语雀卡片，点击链接查看](https://www.yuque.com/ndx6g9/docs_en/rs7gr8g4s538b1n7#MP5s2)
    
    ## 4.1 **Environment configuration **
    ### 4.1.1 **Pre-dependency**
    + **OpenSPG-Server **
    
    **KAG relies on OpenSPG-Server for metadata management and image storage services. Refer to the first and second parts of this document to complete the server deployment.**
    
    ### 4.1.2 **Virtual Environment Installation**
    ```bash
    # Install conda
    # conda installation ：https://docs.anaconda.com/miniconda/
    
    # Install python virtual env：
    $ conda create -n kag-demo python=3.10 && conda activate kag-demo
    ```
    
    ## 4.2 **Code Clone**
    ```bash
    # code clone：
    $ git clone https://github.com/OpenSPG/KAG.git
    
    # KAG installation: 
    $ cd ./KAG && pip install -e .
    
    # confirmation
    $ knext --version
    $ knext --help
    Usage: knext [OPTIONS] COMMAND [ARGS]...
    
    Options:
      --version  Show the version and exit.
      --help     Show this message and exit.
    
    Commands:
      project   Project client.
      reasoner  Reasoner client.
      schema    Schema client.
      thinker   Thinker client.
    ```
    
    ## 4.3 **Create a knowledge base**
    ### 4.3.1 **New Project **
    + **enter the project examples Directory**
    
    ```bash
    $ cd kag/examples
    ```
    
    + **edit Project Configuration**
    
    ```bash
    $ vim ./example_config.yaml
    ```
    
    ```yaml
    #------------project configuration start----------------#
    openie_llm: &openie_llm
      api_key: key
      base_url: https://api.deepseek.com
      model: deepseek-chat
      type: maas
    
    chat_llm: &chat_llm
      api_key: key
      base_url: https://api.deepseek.com
      model: deepseek-chat
      type: maas
    
    vectorize_model: &vectorize_model
      api_key: key
      base_url: https://api.siliconflow.cn/v1/
      model: BAAI/bge-m3
      type: openai
      vector_dimensions: 1024
    vectorizer: *vectorize_model
    
    log:
      level: INFO
    
    project:
      biz_scene: default
      host_addr: http://127.0.0.1:8887
      id: "1"
      language: en
      namespace: TwoWikiTest
    #------------project configuration end----------------#
    
    #------------kag-builder configuration start----------------#
    kag_builder_pipeline:
      chain:
        type: unstructured_builder_chain # kag.builder.default_chain.DefaultUnstructuredBuilderChain
        extractor:
          type: schema_free_extractor # kag.builder.component.extractor.schema_free_extractor.SchemaFreeExtractor
          llm: *openie_llm
          ner_prompt:
            type: default_ner # kag.builder.prompt.default.ner.OpenIENERPrompt
          std_prompt:
            type: default_std # kag.builder.prompt.default.std.OpenIEEntitystandardizationdPrompt
          triple_prompt:
            type: default_triple # kag.builder.prompt.default.triple.OpenIETriplePrompt
        reader:
          type: dict_reader # kag.builder.component.reader.dict_reader.DictReader
        post_processor:
          type: kag_post_processor # kag.builder.component.postprocessor.kag_postprocessor.KAGPostProcessor
          similarity_threshold: 0.9
        splitter:
          type: length_splitter # kag.builder.component.splitter.length_splitter.LengthSplitter
          split_length: 100000
          window_length: 0
        vectorizer:
          type: batch_vectorizer # kag.builder.component.vectorizer.batch_vectorizer.BatchVectorizer
          vectorize_model: *vectorize_model
        writer:
          type: kg_writer # kag.builder.component.writer.kg_writer.KGWriter
      num_threads_per_chain: 1
      num_chains: 16
      scanner:
        type: 2wiki_dataset_scanner # kag.builder.component.scanner.dataset_scanner.MusiqueCorpusScanner
    #------------kag-builder configuration end----------------#
    
    #------------kag-solver configuration start----------------#
    search_api: &search_api
      type: openspg_search_api #kag.solver.tools.search_api.impl.openspg_search_api.OpenSPGSearchAPI
    
    graph_api: &graph_api
      type: openspg_graph_api #kag.solver.tools.graph_api.impl.openspg_graph_api.OpenSPGGraphApi
    
    exact_kg_retriever: &exact_kg_retriever
      type: default_exact_kg_retriever # kag.solver.retriever.impl.default_exact_kg_retriever.DefaultExactKgRetriever
      el_num: 5
      llm_client: *chat_llm
      search_api: *search_api
      graph_api: *graph_api
    
    fuzzy_kg_retriever: &fuzzy_kg_retriever
      type: default_fuzzy_kg_retriever # kag.solver.retriever.impl.default_fuzzy_kg_retriever.DefaultFuzzyKgRetriever
      el_num: 5
      vectorize_model: *vectorize_model
      llm_client: *chat_llm
      search_api: *search_api
      graph_api: *graph_api
    
    chunk_retriever: &chunk_retriever
      type: default_chunk_retriever # kag.solver.retriever.impl.default_fuzzy_kg_retriever.DefaultFuzzyKgRetriever
      llm_client: *chat_llm
      recall_num: 10
      rerank_topk: 10
    
    kag_solver_pipeline:
      memory:
        type: default_memory # kag.solver.implementation.default_memory.DefaultMemory
        llm_client: *chat_llm
      max_iterations: 3
      reasoner:
        type: default_reasoner # kag.solver.implementation.default_reasoner.DefaultReasoner
        llm_client: *chat_llm
        lf_planner:
          type: default_lf_planner # kag.solver.plan.default_lf_planner.DefaultLFPlanner
          llm_client: *chat_llm
          vectorize_model: *vectorize_model
        lf_executor:
          type: default_lf_executor # kag.solver.execute.default_lf_executor.DefaultLFExecutor
          llm_client: *chat_llm
          force_chunk_retriever: true
          exact_kg_retriever: *exact_kg_retriever
          fuzzy_kg_retriever: *fuzzy_kg_retriever
          chunk_retriever: *chunk_retriever
          merger:
            type: default_lf_sub_query_res_merger # kag.solver.execute.default_sub_query_merger.DefaultLFSubQueryResMerger
            vectorize_model: *vectorize_model
            chunk_retriever: *chunk_retriever
      generator:
        type: default_generator # kag.solver.implementation.default_generator.DefaultGenerator
        llm_client: *chat_llm
        generate_prompt:
          type: resp_simple # kag/examples/2wiki/solver/prompt/resp_generator.py
      reflector:
        type: default_reflector # kag.solver.implementation.default_reflector.DefaultReflector
        llm_client: *chat_llm
    
    #------------kag-solver configuration end----------------#
    ```
    
    Please remember to fill in the `api_key` field of `chat_llm`and `openie_llm` with your DeepSeek API key, and fill in the `api_key` field of `vectorize_model` with your SiliconFlow API key.
    
    + **create Project **(One-to-one mapping with the knowledge base in the product): 
    
    ```bash
    $ knext project create --config_path ./example_config.yaml
    ```
    
    + **directory Initialization **
    
    After creating a project, a directory with the same name as the `namespace` field in the project configuration (e.g., `TwoWikiTest` in the example) will be created under the kag/examples directory, and the KAG project code framework will be initialized.
    
    
    
    Users can modify one or more of the following files to customize the business-specific knowledge graph construction and reasoning-based question answering.
    
    ```yaml
    TwoWikiTest
      ├── builder
      │   ├── data
      │   ├── indexer.py
      │   └── prompt
      │   		├── ner.py
      │   		├── std.py
      │   		└── tri.py
      ├── kag_config.cfg
      ├── reasoner
      ├── schema
      │   ├── TwoWikiTest.schema
      └── solver
          ├── evaForHotpotqa.py
          └── prompt
              ├── logic_form_plan.py
              └── resp_generator.py
    ```
    
    ### 4.3.2 **update the project (Optional)**
    If there are any configuration changes, you may refer to the content of this section to update the configuration  on the server side.
    
    + **enter the project examples Directory **
    
    ```bash
    $ cd kag/examples/TwoWikiTest
    ```
    
    + **edit Project Configuration**
    
    Note: Due to the varying usage of different representation models, it is not recommended to update the `vectorizer` configuration after the project has been created. If there is a need to update the `vectorizer` configuration, it is advised to create a new project instead.
    
    ```bash
    $ vim ./kag_config.yaml
    ```
    
    Once again, please ensure that all `api-key` fields have been correctly filled in.
    
    + **run command: **
    
    ```bash
    $ knext project update --proj_path ./
    ```
    
    ## 4.4 **Import documents **
    + **Enter the project directory**
    
    ```bash
    $ cd kag/examples/TwoWikiTest
    ```
    
    + **Obtain the corpus file**
    
    The test corpus data for the 2wiki dataset is located at `examples/2wiki/builder/data/2wiki_corpus.json`, which contains 6,119 documents and 1,000 question-answer pairs. To quickly run through the entire process, there is also a `2wiki_sub_corpus.json` file in the directory, which includes only 3 documents. We will use this smaller dataset as an example for experimentation.  
    
    Copy the corpus file to the corresponding directory in the `TwoWikiTest` project:
    
    ```bash
    $ cp ../2wiki/builder/data/2wiki_sub_corpus.json builder/data/
    ```
    
    + **Edit schema(Optional)  **
    
    User can refere to [Declarative Schema](https://openspg.yuque.com/ndx6g9/cwh47i/fiq6zum3qtzr7cne) to customize `schema/TwoWikiTest.schema` file.
    
    + **Submit the updated schema to SPG server**
    
    ```bash
    $ knext schema commit
    ```
    
    + **Execute the build task.**
    
    Define the build script in the `builder/indexer.py` file:
    
    ```bash
    $ vim ./builder/indexer.py
    ```
    
    ```python
    import os
    import logging
    from kag.common.registry import import_modules_from_path
    
    from kag.builder.runner import BuilderChainRunner
    
    logger = logging.getLogger(__name__)
    
    
    def buildKB(file_path):
        from kag.common.conf import KAG_CONFIG
    
        runner = BuilderChainRunner.from_config(KAG_CONFIG.all_config["kag_builder_pipeline"])
        runner.invoke(file_path)
    
        logger.info(f"\n\nbuildKB successfully for {file_path}\n\n")
    
    
    if __name__ == "__main__":
        import_modules_from_path(".")
        dir_path = os.path.dirname(__file__)
        # Set the `file_path` to the path of the previously prepared corpus file.
        file_path = os.path.join(dir_path, "data/2wiki_sub_corpus.json")
    
        buildKB(file_path)
    
    ```
    
    + **Builder Chain running **
    
    Run the`indexer.py` script to complete the graph construction of unstructured data
    
    ```bash
    $ cd builder
    $ python ./indexer.py
    ```
    
    Once the build script is launched, a checkpoint directory for the task will be generated under the current working directory(i.e., `./builder`), recording the checkpoints and statistical information of the construction pipeline.
    
    ```shell
    builder
    ├── ckpt
    │   ├── chain
    │   ├── extractor
    │   ├── kag_checkpoint_0_1.ckpt
    │   ├── postprocessor
    │   ├── reader
    │   └── splitter
    ├── data
    │   ├── 2wiki_sub_corpus.json
    ├── indexer.py
    ```
    
    Use the following command to view the statistical information of the extraction task, such as how many nodes/edges are extracted from each document:
    
    ```shell
    $ less ckpt/kag_checkpoint_0_1.ckpt
    ```
    
    ![1736226618550-dbae6386-7d6d-4001-9baa-cd639d93d2df.png](./img/LPB7FOFlrQHK4jpL/1736226618550-dbae6386-7d6d-4001-9baa-cd639d93d2df-041196.png)
    
    Use the following command to check how many document entries have been successfully processed and written to the graph database:
    
    ```shell
    $ wc -l ckpt/kag_checkpoint_0_1.ckpt
    ```
    
    ![1736226642797-63314b7a-db45-44ca-8aea-cacceab619ed.png](./img/LPB7FOFlrQHK4jpL/1736226642797-63314b7a-db45-44ca-8aea-cacceab619ed-183422.png)
    
    The KAG framework provides a checkpoint-based resumption feature. If the task is interrupted due to program errors or other external reasons (such as insufficient LLM balance), you can re-execute `indexer.py`. KAG will automatically detect the checkpoint files and reuse the existing results.
    
    + **Result Inspection**
    
    Currently, KAG offers [knowledge exploration](https://openspg.yuque.com/ndx6g9/cwh47i/mzq74eaynm4rqx4b) capabilities on the product side, along with the corresponding [API documentation](https://openspg.yuque.com/ndx6g9/cwh47i/qvbgge62p7argtd2).
    
    ![1736224942237-ae839ce6-9537-4062-a96a-208d6b189e98.png](./img/LPB7FOFlrQHK4jpL/1736224942237-ae839ce6-9537-4062-a96a-208d6b189e98-528085.png)
    
    
    
    ![1736224895758-e147c7fc-e3b6-449c-8a09-e39f85171e56.png](./img/LPB7FOFlrQHK4jpL/1736224895758-e147c7fc-e3b6-449c-8a09-e39f85171e56-535048.png)
    
    ## 4.5 **Reasoning Questions and Answers**
    + **Enter the project directory**
    
    ```shell
    cd kag/examples/TwoWikiTest
    ```
    
    + **Edit the QA script**
    
    ```bash
    $ vim ./solver/qa.py
    ```
    
    Paste the following content into `qa.py`.
    
    ```python
    import json
    import logging
    import os
    import time
    from concurrent.futures import ThreadPoolExecutor, as_completed
    
    from tqdm import tqdm
    
    from kag.common.benchmarks.evaluate import Evaluate
    from kag.solver.logic.solver_pipeline import SolverPipeline
    from kag.common.conf import KAG_CONFIG
    from kag.common.registry import import_modules_from_path
    
    from kag.common.checkpointer import CheckpointerManager
    
    logger = logging.getLogger(__name__)
    
    
    class EvaFor2wiki:
        """
        init for kag client
        """
    
        def __init__(self):
            pass
    
        """
            qa from knowledge base,
        """
    
        def qa(self, query):
            resp = SolverPipeline.from_config(KAG_CONFIG.all_config["kag_solver_pipeline"])
            answer, traceLog = resp.run(query)
    
            logger.info(f"\n\nso the answer for '{query}' is: {answer}\n\n")
            return answer, traceLog
    
    if __name__ == "__main__":
        import_modules_from_path("./prompt")
        evalObj = EvaFor2wiki()
    
        evalObj.qa("Which Stanford University professor works on Alzheimer's?")
    ```
    
    + **Execute the QA script:**
    
    ```bash
    $ cd solver
    $ python ./qa.py
    ```
    
    ## 4.6 **Other built-in cases**
    you can enter the kag/examples directory to experience the cases brought in the source code. 
    
    [Musique(Multi-hop Q&A)](https://openspg.yuque.com/ndx6g9/cwh47i/gfxqv13g1tks9cy7)
    
    [Twowiki(Multi-hop Q&A)](https://openspg.yuque.com/ndx6g9/cwh47i/nqllvpvkv2c2ny8g)
    
    [Hotpotqa(Multi-hop Q&A)](https://openspg.yuque.com/ndx6g9/cwh47i/pa09hxroyt48fv4z)
    
    [Risk Mining Knowledge Graph](https://openspg.yuque.com/ndx6g9/cwh47i/merirblg8d1cbgkg)
    
    [Enterprise Supply Chain Knowledge Graph](https://openspg.yuque.com/ndx6g9/cwh47i/tmebvz9z74rc4szu)
    
    [Medical Knowledge Graph](https://ope
    nspg.yuque.com/ndx6g9/cwh47i/bkvfaunnpl65rsrk)
    
    
    
    ## 4.7、FAQ
    Please refer to the [FAQ](https://openspg.yuque.com/ndx6g9/cwh47i/thg06a4winnimtq2) section for details on how to view checkpoint content and customize extraction & QA task-related information.
风险细节：
    敏感类型：URL地址(url)
    敏感细节：{hitWord=https://www.deepseek.com/)}
{hitWord=https://chat.openai.com/)}
{hitWord=https://docs.siliconflow.cn/api-reference/embeddings/create-embeddings)}
{hitWord=https://chat.openai.com/)}
{hitWord=http://127.0.0.1:8887)}
{hitWord=https://openspg.yuque.com/ndx6g9/0.5/nbb1bn3wegwue6yo?inner=zb7FU)}
{hitWord=https://openspg.yuque.com/ndx6g9/docs_en/nmq2aq4s11b6mgxx).}
{hitWord=https://api.siliconflow.cn/v1}
{hitWord=https://openspg.yuque.com/ndx6g9/docs_en/tx0gd5759hg4xi56).}
{hitWord=https://api.deepseek.com}
{hitWord=https://openspg.yuque.com/attachments/yuque/0/2025/txt/32480677/1735889862217-c48e42ce-696d-45d5-8903-08e84bf5f841.txt)}
{hitWord=https://openspg.yuque.com/attachments/yuque/0/2025/txt/32480677/1735889862633-c2cc2584-b4d7-4990-8398-3a73dd95a8a3.txt)}
{hitWord=https://openspg.yuque.com/attachments/yuque/0/2025/txt/32480677/1735889862788-86c08705-ca8a-4f01-bb69-f87d60c4f736.txt)}
{hitWord=https://openspg.yuque.com/ndx6g9/cwh47i/mzq74eaynm4rqx4b)}
{hitWord=https://openspg.yuque.com/ndx6g9/0.5/nbb1bn3wegwue6yo?inner=S8PoP)}
{hitWord=https://www.yuque.com/ndx6g9/docs_en/rs7gr8g4s538b1n7#MP5s2)}
{hitWord=https://docs.anaconda.com/miniconda/}
风险内容位置：
    CommitId: 2911f9fb9d07e718b153121318ef12824a30a299
    文件名：website/docs/en/UserGuide/Customextensions/Schemacustomization.md
    文件行：1~367
风险内容：
    ---
    sidebar_position: 1
    ---
    
    # Schema customization
    
    KAG supports custom business schemas to influence the graph construction and graph inference processes.   
    KAG supports the construction of knowledge graphs based on business data (both structured and unstructured) and expert rules. The schema enforces constraints on the graph construction for structured data and expert rules, while for unstructured data, the schema guides the large model in performing knowledge extraction. 
    
    # 1 SPG Schema Syntax 
    For a description of the OpenSPG schema syntax, please refer to [Declarative Schema](https://openspg.yuque.com/ndx6g9/cwh47i/fiq6zum3qtzr7cne) [KGDSL](https://openspg.yuque.com/ndx6g9/cwh47i/hgqqkt8h9hh333tl).
    
    For specific schema examples in various business scenarios, please refer to [Medicine.schema](https://github.com/OpenSPG/KAG/blob/master/kag/examples/medicine/schema/Medicine.schema), [RiskMining.schema](https://github.com/OpenSPG/KAG/blob/master/kag/examples/riskmining/schema/RiskMining.schema), [SupplyChain.schema](https://github.com/OpenSPG/KAG/blob/master/kag/examples/supplychain/schema/SupplyChain.schema), [HotpotQA.schema](https://github.com/OpenSPG/KAG/blob/master/kag/examples/hotpotqa/schema/HotpotQA.schema), [TwoWiki.schema](https://github.com/OpenSPG/KAG/blob/master/kag/examples/2wiki/schema/TwoWiki.schema), [MuSiQue.schema](https://github.com/OpenSPG/KAG/blob/master/kag/examples/musique/schema/MuSiQue.schema), etc.
    
    ## 1.1 RiskMining.schema 
    For example, in the RiskMining.schema file: TaxOfRiskUser is a concept that describes the classification of risk users. Company is an entity type that describes a business. Person is an entity type that describes an individual. The Person entity type can define attributes and relationships. The difference is that attribute values can be of basic types (such as Text, Integer, Date, etc.) or concept types, while relationship types point to another entity type.
    
    The existence of relationships can be expressed using DSL (Domain-Specific Language) rules. For example, Person-developed->App defines a "person-develops-app" relationship. This relationship is not explicitly present in the raw data but is derived from the raw data (e.g., Person-hasDevice->Device, Device-install->App) after multiple processing steps (such as aggregation and statistics based on business rules). 
    
    In practice, raw data and business rules are often updated frequently. It is essential to ensure that when the raw data is updated, the corresponding inference results from the business rules are also updated. The relationships expressed using DSL rules in the SPG schema are generated through real-time computation during N-degree inference, which effectively meets this requirement.
    
    ```bash
    TaxOfRiskUser(风险用户): ConceptType
    	hypernymPredicate: isA
    	
    TaxOfRiskApp(风险应用): ConceptType
    	hypernymPredicate: isA
    
    Cert(证书): EntityType
    	properties:
    		certNum(证书编号): Text
    		
    Company(企业): EntityType
    	properties:
    		hasPhone(电话号码): Text
    	relations:
    		hasCert(拥有证书): Cert
    		holdShare(持股): Company
    		
    App(应用): EntityType
    	properties:
    		riskMark(风险标记): Text
    		useCert(使用证书): Cert
    		IND#belongTo(属于): TaxOfRiskApp
    
    Person(自然人): EntityType
    	properties:
    		age(年龄): Integer
    		hasPhone(电话号码): Text
    		IND#belongTo(属于): TaxOfRiskUser
    	relations:
    		hasDevice(拥有设备): Device
    		hasCert(拥有证书): Cert
    		holdShare(持股): Company
    		fundTrans(转账关系): Person
    			properties:
    				transDate(交易日期): Text
    				transAmt(交易金额): Integer
    		developed(开发): App
    			rule: [[
    			        Define (s:Person)-[p:developed]->(o:App) {
    				        STRUCTURE {
    				          	(s)-[:hasDevice]->(d:Device)-[:install]->(o)
    				        }
    						CONSTRAINT {
    						   deviceNum = group(s,o).count(d)
    						   R1("设备超过5"): deviceNum > 5
    						}
    					}
          			  ]]
        release(发布): App
    			rule: [[
    			        Define (s:Person)-[p:release]->(o:App) {
                            STRUCTURE {
    						    (s)-[:holdShare]->(c:Company),
    						    (c)-[:hasCert]->(cert:Cert)<-[useCert]-(o)
    					    }
    					    CONSTRAINT {
    					    }
    					}
          			  ]]
    ```
    
    ## 1.2 HotpotQA.schema 
    Datasets like HotpotQA and Musique contain a wide and complex range of information, making it difficult to define a schema that can accurately accommodate all entity instances. For example, Jay Chou as an actor and Cao Cao as a military strategist and politician, although both are categorized under the Person (individual) entity type, have significantly different associated attributes and relationships.   
    	Unlike the strong schema constraints imposed in the construction of structured data graphs, for open-domain data, KAG (Knowledge Augmentation Graph) adopts a semi-schema approach to guide large models in knowledge extraction. This approach aims to balance the accuracy, logical rigor, and contextual completeness of the knowledge.   
    	For instance, in the hotpotqa.schema file, for open-domain datasets, the Person entity type only predefines basic attributes such as desc (entity description) and semanticType (entity semantic type, also known as sub-type), along with built-in attributes like id and name. Additional attributes and relationships are extracted through a combination of the schema and prompts, guiding the large model to complete the knowledge extraction. 
    
    ```bash
    Chunk(文本块): EntityType
         properties:
            content(内容): Text
                index: TextAndVector
    
    Person(人物): EntityType
         properties:
            desc(描述): Text
                index: TextAndVector
            semanticType(语义类型): Text
                index: Text
    
    Transport(运输): EntityType
         properties:
            desc(描述): Text
                index: TextAndVector
            semanticType(语义类型): Text
                index: Text
    
    Works(作品): EntityType
         properties:
            desc(描述): Text
                index: TextAndVector
            semanticType(语义类型): Text
                index: Text
    ```
    
    The built-in schema in the KAG product, designed for open-domain datasets, may not meet the expectations for domain-specific knowledge extraction tasks. Therefore, developers need to design their own domain-specific schemas, which can be used as part of the prompts to guide the large model in completing the knowledge extraction.   
    	Additionally, the KAG team continuously releases domain-specific schemas for users to reference. 
    
    # 2 Schema File Submission and Update 
    For example, in the risk mining scenario, the schema files are typically located in the `./${bizScene}/schema/` directory. This directory includes SPG (Structured Property Graph) format schema files as well as rule files. 
    
    ```bash
    examples
    ├──riskmining
        ├── builder
        ├── reasoner
        ├── schema
        │   ├── RiskMining.schema
        │   └── concept.rule
        └── solver
    ```
    
    ## 2.1 Schema File Submission
    When submitting schema files, they undergo strict format validation, including checks for indentation spaces and keyword naming. If no errors are reported during submission, the defined schema can be used in subsequent processes. 
    
    ```bash
    $ cd examples/riskmining/
    # commit schema file
    $ knext schema --help
    Usage: knext schema [OPTIONS] COMMAND [ARGS]...
    
      Schema client.
    
    Options:
      --help  Show this message and exit.
    
    Commands:
      commit            Commit local schema and generate schema helper.
      reg_concept_rule  Register a concept rule according to DSL file.
      
    $ knext schema commit
    
    # commit rule file
    $ knext schema reg_concept_rule --help
    Usage: knext schema reg_concept_rule [OPTIONS]
    
      Register a concept rule according to DSL file.
    
    Options:
      --file TEXT  Path of DSL file.
      --help       Show this message and exit.
    $ knext schema reg_concept_rule ./schema/concept.rule
    ```
    
    ## 2.2 Schema File update
    After updating the schema files, you can resubmit them to the server using the knext command. 
    
    # 3 How Schema works
    ## 3.1 Knowledge Extraction from Unstructured Data 
    + **NER in public domain**
    
    ```bash
    {
        "instruction": "You're a very effective entity extraction system. Please extract all the entities that are important for knowledge build and question, along with type, category and a brief description of the entity. The description of the entity is based on your OWN KNOWLEDGE AND UNDERSTANDING and does not need to be limited to the context. the entity's category belongs taxonomically to one of the items defined by schema, please also output the category. Note: Type refers to a specific, well-defined classification, such as Professor, Actor, while category is a broader group or class that may contain more than one type, such as Person, Works. Return an empty list if the entity type does not exist. Please respond in the format of a JSON string.You can refer to the example for extraction.",
        "schema": $schema,
        "example": [
            {
                "input": "The Rezort\nThe Rezort is a 2015 British zombie horror film directed by Steve Barker and written by Paul Gerstenberger.\n It stars Dougray Scott, Jessica De Gouw and Martin McCann.\n After humanity wins a devastating war against zombies, the few remaining undead are kept on a secure island, where they are hunted for sport.\n When something goes wrong with the island's security, the guests must face the possibility of a new outbreak.",
                "output": [
                            {
                                "entity": "The Rezort",
                                "type": "Movie",
                                "category": "Works",
                                "description": "A 2015 British zombie horror film directed by Steve Barker and written by Paul Gerstenberger."
                            },
                            {
                                "entity": "British",
                                "type": "Nationality",
                                "category": "GeographicLocation",
                                "description": "Great Britain, the island that includes England, Scotland, and Wales."
                            },
                            {
                                "entity": "Steve Barker",
                                "type": "Director",
                                "category": "Person",
                                "description": "Steve Barker is an English film director and screenwriter."
                            },
                            {
                                "entity": "Paul Gerstenberger",
                                "type": "Writer",
                                "category": "Person",
                                "description": "Paul is a writer and producer, known for The Rezort (2015), Primeval (2007) and House of Anubis (2011)."
                            },
                            {
                                "entity": "Dougray Scott",
                                "type": "Actor",
                                "category": "Person",
                                "description": "Stephen Dougray Scott (born 26 November 1965) is a Scottish actor."
                            },
                            {
                                "entity": "Jessica De Gouw",
                                "type": "Actor",
                                "category": "Person",
                                "description": "Jessica Elise De Gouw (born 15 February 1988) is an Australian actress. "
                            },
                            {
                                "entity": "Martin McCann",
                                "type": "Actor",
                                "category": "Person",
                                "description": "Martin McCann is an actor from Northern Ireland. In 2020, he was listed as number 48 on The Irish Times list of Ireland's greatest film actors"
                            }
                        ]
            }
        ],
        "input": "$input"
    }
    ```
    
    + **NER in medicine**
    
    ```bash
    {
            "instruction": "你是命名实体识别的专家。请从输入中提取与模式定义匹配的实体。如果不存在该类型的实体，请返回一个空列表。请以JSON字符串格式回应。你可以参照example进行抽取。",
            "schema": $schema,
            "example": [
                {
                    "input": "烦躁不安、语妄、失眠酌用镇静药，禁用抑制呼吸的镇静药。\n3.并发症的处理经抗菌药物治疗后，高热常在24小时内消退，或数日内逐渐下降。\n若体温降而复升或3天后仍不降者，应考虑SP的肺外感染。\n治疗：接胸腔压力调节管＋吸引机负压吸引水瓶装置闭式负压吸引宜连续，如经12小时后肺仍未复张，应查找原因。",
                    "output": [
                            {"entity": "烦躁不安", "category": "Symptom"},
                            {"entity": "语妄", "category": "Symptom"},
                            {"entity": "失眠", "category": "Symptom"},
                            {"entity": "镇静药", "category": "Medicine"},
                            {"entity": "肺外感染", "category": "Disease"},
                            {"entity": "胸腔压力调节管", "category": "MedicalEquipment"},
                            {"entity": "吸引机负压吸引水瓶装置", "category": "MedicalEquipment"},
                            {"entity": "闭式负压吸引", "category": "SurgicalOperation"}
                        ]
                }
            ],
            "input": "$input"
        }
    ```
    
    ## 3.2 Knowledge Build for Structured Data
    + **schema of EntityType**
    
    ```python
    Company(企业): EntityType
    	properties:
    		hasPhone(电话号码): Text
    	relations:
    		hasCert(拥有证书): Cert
    		holdShare(持股): Company
    ```
    
    + **data to build**
    
    ```python
    id,name,hasPhone
    0,**娱乐****公司,150****3237
    1,**娱乐**公司,133****4755
    2,**娱乐***公司,152****7817
    3,哈尔*工***技术产**发**限公司,196****5023
    4,中国**业建*股*限公司,137****3517
    5,莲花*康**集**份限公司,190****4555
    6,深圳***智能*气**限公司,132****0163
    7,中信*投证***限公司,133****1366
    8,航天彩**人*股*限公司,156****6507
    9,供销*集*团**限公司,156****3837
    ```
    
    + **BuilderChain**
    
    ```python
    class RiskMiningEntityChain(BuilderChainABC):
        def __init__(self, spg_type_name: str):
            super().__init__()
            self.spg_type_name = spg_type_name
    
        def build(self, **kwargs):
            source = CSVReader(output_type="Dict")
            mapping = SPGTypeMapping(spg_type_name=self.spg_type_name)
            vectorizer = BatchVectorizer()
            sink = KGWriter()
    
            chain = source >> mapping >> vectorizer >> sink
            return chain
    
    RiskMiningEntityChain(spg_type_name="Company").invoke(os.path.join(file_path, "data/Company.csv"))
    ```
    
    SPGTypeMapping will parse the fieldName from the CSV file and map it to the properties defined in the EntityType. 
    
    ## 3.3 KAG Solver
    To more clearly describe the technical implementation of graph inference-based question answering, this document categorizes it from a data modeling perspective into the following two types: 
    
    + **Inference with Existing Data Modeling:** This type of inference is for structured data that has a clear data schema.
    + **Inference without Data Modeling: **This type of inference is for unstructured data that lacks a clear data schema.
    
    The technical implementation of these two types of inference in graph-based question answering is detailed below. In graph inference-based question answering, we distinguish between inference on structured data and inference on unstructured data. 
    
    ### 3.3.1 **Inference with Existing Data Modeling**
    Inference for existing data modeling primarily targets structured data, which is characterized by a clear data schema. This data is usually smaller in scale but may contain complex business logic. When applying large models to such data, the main challenges are: 
    
    + Data Scale Limitation: Large models cannot directly handle massive amounts of structured data.
    + Insufficient Knowledge Dependency: Large models lack sufficient knowledge about the underlying data.
    
    Given these two issues, the most challenging problem in implementing a question-answering service on structured data is how to convert natural language into graph query steps. To solve this problem, the Plan module must understand how the underlying graph data is stored and expressed. SPG-Schema serves as the overall description of our graph knowledge, and it can help the Plan module understand the graph knowledge representation.   
    To express knowledge more concisely and precisely, SPG-Schema includes a logical rule layer that allows domain experts to decouple specific data and business expressions using a Domain-Specific Language (DSL). For more details, refer to the content in the SPG white paper.   
    Below is an example from RiskMining:   
    In RiskMining, the most important requirement is to determine the risk category of a user. Therefore, the schema defines a logical inference relationship (IND#belongTo). 
    
    ```plain
    TaxOfRiskUser(风险用户): ConceptType
    	hypernymPredicate: isA
    
    Person(自然人): EntityType
    	properties:
    		age(年龄): Integer
    		hasPhone(电话号码): Text
    		IND#belongTo(属于): TaxOfRiskUser
    ```
    
    The specific logical expressions are stored in concept.rule, and we will not elaborate on them here. During the planning phase, we provided the large model with the following example:
    
    ```json
    {
      "query": "Is ZhangSan a developer of a gambling app?",
        "answer": "Step1:Query which TaxOfRiskUser does zhangsan belongTo
                  Action1:get_spo(s=s1:Person[ZhangSan], p=p1:belongTo, o=o1:TaxOfRiskUser)
                  Output:o1
                  Action2:get(o1)"
    }
    ```
    
    The relationship (Person)-[belongTo]->(TaxOfRiskUser) is a logical edge defined in the schema. When the executor processes this logic form expression, it converts it into the following query statement: 
    
    ```json
    MATCH 
      (s1:Person)-[p1:belongTo]->(o1:TaxOfRiskUser)
    WHERE
      s1.id="ZhangSan"
    RETURN
      o1
    ```
    
    This triggers the OpenSPG inference engine to return Zhang's risk classification result.   
    On one hand, SPG-Schema enhances the expression of domain-specific relationship attributes through expert rule logic. On the other hand, it reduces the large model's need to understand domain knowledge during the planning phase. By combining these two aspects, we can achieve a question-answering application on structured data. 
    
    ### 3.3.2 Inference without Data Modeling
    Inference without Data Modeling primarily targets unstructured data, which is characterized by the lack of a clear data schema, diverse data formats, and high flexibility. In such scenarios, the system cannot rely on a predefined schema to optimize the planner (Planner) and instead uses a weak schema constraint mechanism to express any type of data through entity types (Entity). 
    
    + **Weak Schema Constraint Mechanism: **In non-data modeling scenarios, OpenSPG expresses any type of data through entity types (Entity) and performs inference planning based on domain knowledge. This design allows the system to adapt to the high flexibility and diversity of unstructured data.
    + **Application of Domain Knowledge:** Even without a data model, the system can be extended as described in section 3.3.1. The system uses a domain knowledge base or predefined rules to convert unstructured data into a structured format, thereby enabling inference.
    + **Dynamic Schema Extension:** In non-data modeling scenarios, the system may need to dynamically extend the schema to accommodate new data types and relationships. This dynamic extension capability allows the system to flexibly handle the high complexity of open domains.
    
风险细节：
    敏感类型：URL地址(url)
    敏感细节：{hitWord=https://openspg.yuque.com/ndx6g9/cwh47i/fiq6zum3qtzr7cne)}
{hitWord=https://openspg.yuque.com/ndx6g9/cwh47i/hgqqkt8h9hh333tl).}
风险内容位置：
    CommitId: 2911f9fb9d07e718b153121318ef12824a30a299
    文件名：website/docs/en/UserGuide/Customextensions/Promptcustomization.md
    文件行：1~772
风险内容：
    ---
    sidebar_position: 2
    ---
    
    # Prompt customization
    
    KAG supports customized prompts to intervene in the effectiveness of graph construction and graph reasoning.
    
    # 1、Basic Concepts of Prompts
    In KAG, the base class for prompts is `kag.interface.PromptABC`, which primarily includes predefined prompt template content (usually task instructions and examples) and the functionality to parse the output strings from LLMs. If users wish to customize prompts, they can inherit the base class. Specifically, the process generally involves the following steps:
    
    ### 1.1、Customize Template Content
    In custom classes, define a variable named `template_{language}` as the prompt template, where `language` represents the supported languages of the prompt, such as `zh` for Chinese and `en` for English. If a class defines both `template_zh` and `template_en` variables, KAG will automatically select the appropriate template based on the `language` field defined in the project configuration.
    
    The template variable should be assigned a value as either a Python dictionary or a JSON string. Below is an example:
    
    + **python dictionary format**
    
    ```python
    template_en: dict = {
        "instruction": "Please extract all dates from the input text.",
        "example": [
            {
                "input": "Jay Chou, born on January 18, 1979, in Taiwan Province.",
                "output": ["January 18, 1979"],
            }
        ],
    }
    ```
    
    + **json string format**
    
    Please note that the content of the `input` field should be represented using `$input`. KAG will automatically replace `$input` with the actual content of the `input` field from the provided parameters. Here is the updated example:
    
    ```python
    template_zh: str = """{
        "instruction": "Please extract all dates from the input text.",
        "example": [
            {
                "input": "Jay Chou, born on January 18, 1979, in Taiwan Province.",
                "output": ["January 18, 1979"],
            }
        ],
    }"""
    
    ```
    
    ### 1.2、Customizing LLM Output Parser
    The code for parsing the LLM output should be implemented in the `parse_response` interface, which generally involves converting the raw output of the large model into the desired format.
    
    Notes:
    
    + Similar to other components of KAG, the prompt class is managed by the registry system. It is highly recommended to first read the [**custom code**](https://openspg.yuque.com/ndx6g9/cwh47i/zwxvp3n4xog1vxsv) chapter to gain a preliminary understanding of the registry mechanism before proceeding with modifications.
    + The KAG framework comes with multiple prompt implementations that are tightly integrated with the knowledge schema. In most cases, users only need to define an appropriate schema and modify the examples in the prompt.
    
    # 2、KG Build Related Prompts
    ## 2.1、**Schema-free Knowledge Extraction**
    ### 2.1.1、Prompt file
    ```python
    ├── __init__.py
    ├── ner.py
    ├── std.py
    └── triple.py
    ```
    
    Taking `Musique` dataset as an example, the KAG builder related prompts for the open-domain dataset are stored in the `kag/builder/prompt/default` directory, which includes three files: `ner.py`, `std.py`, and `triple.py`:
    
    + **ner.py**
    
    The files define the Chinese and English templates for named entity extraction, with the templates presented in JSON string format. `example.input` and `example.output` respectively demonstrate the LLM input and output examples.
    
    Developers can adjust the corresponding examples to enhance the name entity extraction performance. The output format of the LLM is strongly correlated with the data parsing logic. If the data parsing logic has not been modified, it is essential to maintain the stability of the output format in the examples.
    
    ```python
        template_en = """
        {
        "instruction": "You're a very effective entity extraction system. Please extract all the entities that are important for knowledge build and question, along with type, category and a brief description of the entity. The description of the entity is based on your OWN KNOWLEDGE AND UNDERSTANDING and does not need to be limited to the context. the entity's category belongs taxonomically to one of the items defined by schema, please also output the category. Note: Type refers to a specific, well-defined classification, such as Professor, Actor, while category is a broader group or class that may contain more than one type, such as Person, Works. Return an empty list if the entity type does not exist. Please respond in the format of a JSON string.You can refer to the example for extraction.",
        "schema": $schema,
        "example": [
            {
                "input": "The Rezort is a 2015 British zombie horror film directed by Steve Barker and written by Paul Gerstenberger. It stars Dougray Scott, Jessica De Gouw and Martin McCann. After humanity wins a devastating war against zombies, the few remaining undead are kept on a secure island, where they are hunted for sport. When something goes wrong with the island's security, the guests must face the possibility of a new outbreak.",
                "output": [
                            {
                                "name": "The Rezort",
                                "type": "Movie",
                                "category": "Works",
                                "description": "A 2015 British zombie horror film directed by Steve Barker and written by Paul Gerstenberger."
                            },
                            {
                                "name": "2015",
                                "type": "Year",
                                "category": "Date",
                                "description": "The year the movie 'The Rezort' was released."
                            },
                            {
                                "name": "British",
                                "type": "Nationality",
                                "category": "GeographicLocation",
                                "description": "Great Britain, the island that includes England, Scotland, and Wales."
                            },
                            {
                                "name": "Steve Barker",
                                "type": "Director",
                                "category": "Person",
                                "description": "Steve Barker is an English film director and screenwriter."
                            },
                            {
                                "name": "Paul Gerstenberger",
                                "type": "Writer",
                                "category": "Person",
                                "description": "Paul is a writer and producer, known for The Rezort (2015), Primeval (2007) and House of Anubis (2011)."
                            },
                            {
                                "name": "Dougray Scott",
                                "type": "Actor",
                                "category": "Person",
                                "description": "Stephen Dougray Scott (born 26 November 1965) is a Scottish actor."
                            },
                            {
                                "name": "Jessica De Gouw",
                                "type": "Actor",
                                "category": "Person",
                                "description": "Jessica Elise De Gouw (born 15 February 1988) is an Australian actress. "
                            },
                            {
                                "name": "Martin McCann",
                                "type": "Actor",
                                "category": "Person",
                                "description": "Martin McCann is an actor from Northern Ireland. In 2020, he was listed as number 48 on The Irish Times list of Ireland's greatest film actors"
                            }
                        ]
            }
        ],
        "input": "$input"
    }    
            """
    
    ```
    
    + **std.py**
    
    The files define the Chinese and English templates for entity standardization, with the templates presented in JSON string format. `example.input`, `example.named_entities`, and `example.output` respectively demonstrate the LLM input and output examples.
    
    Entity standardization relies on the LLM's understanding of the context as well as its own knowledge base. Standardized entity names can complement the context of the entities, thereby avoiding ambiguity.
    
    ```python
        template_en = """
    {
        "instruction": "The `input` field contains a user provided context. The `named_entities` field contains extracted named entities from the context, which may be unclear abbreviations, aliases, or slang. To eliminate ambiguity, please attempt to provide the official names of these entities based on the context and your own knowledge. Note that entities with the same meaning can only have ONE official name. Please respond in the format of a single JSONArray string without any explanation, as shown in the `output` field of the provided example.",
        "example": {
            "input": "American History.When did the political party that favored harsh punishment of southern states after the Civil War, gain control of the House? Republicans regained control of the chamber they had lost in the 2006 midterm elections.",
            "named_entities": [
                {"name": "American", "category": "GeographicLocation"},
                {"name": "political party", "category": "Organization"},
                {"name": "southern states", "category": "GeographicLocation"},
                {"name": "Civil War", "category": "Keyword"},
                {"name": "House", "category": "Organization"},
                {"name": "Republicans", "category": "Organization"},
                {"name": "chamber", "category": "Organization"},
                {"name": "2006 midterm elections", "category": "Date"}
            ],
            "output": [
                {
                    "name": "American",
                    "category": "GeographicLocation",
                    "official_name": "United States of America"
                },
                {
                    "name": "political party",
                    "category": "Organization",
                    "official_name": "Radical Republicans"
                },
                {
                    "name": "southern states",
                    "category": "GeographicLocation",
                    "official_name": "Confederacy"
                },
                {
                    "name": "Civil War",
                    "category": "Keyword",
                    "official_name": "American Civil War"
                },
                {
                    "name": "House",
                    "category": "Organization",
                    "official_name": "United States House of Representatives"
                },
                {
                    "name": "Republicans",
                    "category": "Organization",
                    "official_name": "Republican Party"
                },
                {
                    "name": "chamber",
                    "category": "Organization",
                    "official_name": "United States House of Representatives"
                },
                {
                    "name": "midterm elections",
                    "category": "Date",
                    "official_name": "United States midterm elections"
                }
            ]
        },
        "input": "$input",
        "named_entities": $named_entities
    }
        """
    
    ```
    
    + **triple.py**
    
    The file defines the Chinese and English templates for SPO (Subject-Predicate-Object) triple extraction, with the templates presented in JSON string format. `example.input`, `example.entity_list` and `example.output` respectively demonstrate the LLM input and output examples.
    
    The instruction specifies that for the SPO extraction results, either the subject or the object must appear in the `entity_list`.
    
    ```python
    template_en = """
    {
        "instruction": "You are an expert specializing in carrying out open information extraction (OpenIE). Please extract any possible relations (including subject, predicate, object) from the given text, and list them following the json format {\"triples\": [[\"subject\", \"predicate\",  \"object\"]]}. If there are none, do not list them..Pay attention to the following requirements:- Each triple should contain at least one, but preferably two, of the named entities in the entity_list.- Clearly resolve pronouns to their specific names to maintain clarity.",
        "entity_list": $entity_list,
        "input": "$input",
        "example": {
            "input": "The RezortThe Rezort is a 2015 British zombie horror film directed by Steve Barker and written by Paul Gerstenberger. It stars Dougray Scott, Jessica De Gouw and Martin McCann. After humanity wins a devastating war against zombies, the few remaining undead are kept on a secure island, where they are hunted for sport. When something goes wrong with the island's security, the guests must face the possibility of a new outbreak.",
            "entity_list": [
                {
                    "name": "The Rezort",
                    "category": "Works"
                },
                {
                    "name": "2015",
                    "category": "Others"
                },
                {
                    "name": "British",
                    "category": "GeographicLocation"
                },
                {
                    "name": "Steve Barker",
                    "category": "Person"
                },
                {
                    "name": "Paul Gerstenberger",
                    "category": "Person"
                },
                {
                    "name": "Dougray Scott",
                    "category": "Person"
                },
                {
                    "name": "Jessica De Gouw",
                    "category": "Person"
                },
                {
                    "name": "Martin McCann",
                    "category": "Person"
                },
                {
                    "name": "zombies",
                    "category": "Creature"
                },
                {
                    "name": "zombie horror film",
                    "category": "Concept"
                },
                {
                    "name": "humanity",
                    "category": "Concept"
                },
                {
                    "name": "secure island",
                    "category": "GeographicLocation"
                }
            ],
            "output": [
                [
                    "The Rezort",
                    "is",
                    "zombie horror film"
                ],
                [
                    "The Rezort",
                    "publish at",
                    "2015"
                ],
                [
                    "The Rezort",
                    "released",
                    "British"
                ],
                [
                    "The Rezort",
                    "is directed by",
                    "Steve Barker"
                ],
                [
                    "The Rezort",
                    "is written by",
                    "Paul Gerstenberger"
                ],
                [
                    "The Rezort",
                    "stars",
                    "Dougray Scott"
                ],
                [
                    "The Rezort",
                    "stars",
                    "Jessica De Gouw"
                ],
                [
                    "The Rezort",
                    "stars",
                    "Martin McCann"
                ],
                [
                    "humanity",
                    "wins",
                    "a devastating war against zombies"
                ],
                [
                    "the few remaining undead",
                    "are kept on",
                    "a secure island"
                ],
                [
                    "they",
                    "are hunted for",
                    "sport"
                ],
                [
                    "something",
                    "goes wrong with",
                    "the island's security"
                ],
                [
                    "the guests",
                    "must face",
                    "the possibility of a new outbreak"
                ]
            ]
        }
    }    
        """
    
    ```
    
    ### 2.1.2、Custom prompt
    To modify the aforementioned prompts, follow these steps:
    
    + **Create the prompt directory**
    
    Create a new `prompt` directory under the project's `builder` directory (e.g., `kag/examples/2wiki/builder/prompt`), and copy the prompt files you wish to modify into this directory.
    
    + **Modify the prompt files**
    
    Customize the prompt class and** modify the registration name.**
    
    ```python
    # Original registration name is `default_ner`
    @PromptABC.register("default_ner")
    class OpenIENERPrompt(PromptABC):
        template_en = {}
    
    # Modified registeration name is `my_ner_prompt`
    @PromptABC.register("my_ner_prompt")
    class OpenIENERPrompt(PromptABC):
        template_en = {}
    ```
    
    + **Modify the configuration file**
    
    In the project configuration file `kag_config.yaml`, change the `type` field of the prompt to the registration name of the custom prompt (i.e., `my_ner_prompt` as mentioned above).
    
    + **import the custom prompt**
    
    In the script where the custom prompt is used (such as the KG build script), import the directory of the custom prompts to register the new prompts.
    
    ```python
    from kag.common.registry import import_modules_from_path
    import_modules_from_path("./prompt") # The path where the custom prompt files are located.
    ```
    
    ### 2.1.3、Prompt Application
    The KAG framework includes a built-in component called `SchemaFreeExtractor`, registered under the name "`schema_free_extractor`". It provides the capability to perform knowledge extraction based on open-domain corpora and user-defined prompts/schemas. Specifically, the extraction process of `SchemaFreeExtractor` consists of the following steps:
    
    1. **Entity Extraction** with `ner_prompt`.  
    2. **Entity Standardization** with `std_prompt`.  
    3. **Triple Extraction** with `triple_prompt`.  
    4. **Aggregating extracted data into a graph**.
    
    
    
    ```python
    class SchemaFreeExtractor(ExtractorABC):
        """
        A class for extracting knowledge graph subgraphs from text using a large language model (LLM).
        Inherits from the Extractor base class.
    
        Attributes:
            llm (LLMClient): The large language model client used for text processing.
            schema (SchemaClient): The schema client used to load the schema for the project.
            ner_prompt (PromptABC): The prompt used for named entity recognition.
            std_prompt (PromptABC): The prompt used for named entity standardization.
            triple_prompt (PromptABC): The prompt used for triple extraction.
            external_graph (ExternalGraphLoaderABC): The external graph loader used for additional NER.
        """
    
        def __init__(
            self,
            llm: LLMClient,
            ner_prompt: PromptABC = None,
            std_prompt: PromptABC = None,
            triple_prompt: PromptABC = None,
            external_graph: ExternalGraphLoaderABC = None,
        ):
            """
            Initializes the KAGExtractor with the specified parameters.
    
            Args:
                llm (LLMClient): The large language model client.
                ner_prompt (PromptABC, optional): The prompt for named entity recognition. Defaults to None.
                std_prompt (PromptABC, optional): The prompt for named entity standardization. Defaults to None.
                triple_prompt (PromptABC, optional): The prompt for triple extraction. Defaults to None.
                external_graph (ExternalGraphLoaderABC, optional): The external graph loader. Defaults to None.
            """
    
    ```
    
    ## 2.2、**Schema-constraint Knowledge Extraction**
    In the schema-free knowledge extraction pipeline, we exclusively employ the entity types specified within the schema, with the attributes for each entity type being predetermined (e.g., `name`, `type`, `category` and `description`). This methodology imposes fewer restrictions on the extraction model, thereby allowing the LLMto identify a greater number of key entities.
    
    However, in specific vertical domains, there may be a need for more granular entity extraction, such as distinguishing entities with unique attributes. Furthermore, beyond entities, there is also an objective to extract events that involve multiple subject and object entities. To accommodate such requirements, the KAG framework offers a schema-constrained knowledge extraction capability. By reading and parsing a user-defined schema file, it directs the LLM to execute entity and event extraction in alignment with the schema's specifications.
    
    For instance, a schema file designed for a news knowledge graph could be structured as follows:
    
    ```yaml
    namespace News
    
    Chunk(文本块): EntityType
         properties:
            content(内容): Text
                index: TextAndVector
    
    Date(日期): EntityType
         properties:
            info(信息): Text
                index: TextAndVector
    
    GeographicLocation(地理位置): EntityType
         properties:
            type(Type): Text
                desc: The specific type of geographic location, such as country, region, province, or city.
    
    Entity(实体): EntityType
         desc: A named object or concept with specific meaning, such as a person, organization, product, etc.
         properties:
             type(Type): Text
    
    NewsEvent(新闻事件): EventType
         properties:
           time(时间): Date
               desc: The specific point or period of time when the event occurred.
           location(地点): GeographicLocation
               desc: The specific location or area where the event took place.
           subject(主体): Entity
               desc: The subject of the event, i.e., the main participant of the event.
           relation(关系): Text
               desc: The name of the relationship, i.e., the main action or behavior of the event.
           object(客体): Entity
               desc: The object of the event, i.e., the secondary participant or target of the event.
               constraint: MultiValue
           cause(原因): Text
               desc: The cause or motivation behind the event.
           process(过程): Text
               desc: The process or steps of the event.
           outcome(结果): Text
               desc: The result or impact generated after the event.
           context(背景): Text
               desc: The background or environment in which the event occurred.
           impact(影响): Text
               desc: The impact of the event on relevant parties or society.
    ```
    
    The aforementioned schema not only defines entities and their properties but also specifies the `NewsEvent` under the EventType. This type includes several attributes, some of which are simple text types, such as `cause` and `process`, while others are entity types defined within the schema, such as `time`, `location` and `subject`. Additionally, certain attributes incorporate `MultiValue` constraints, such as `object`.
    
    The schema-constrained knowledge extraction approach can instruct the LLM to perform extraction tasks based on the constraints defined in the schema. Below is an example of a possible input and output for event extraction:
    
    ```yaml
    {
      "input": "On the afternoon of August 27th, Beijing time, U.S. National Security Advisor Jake Sullivan arrived in Beijing by plane, beginning his first visit to China during his tenure. The host noted that almost all members of the accompanying delegation could speak Chinese, and Sullivan himself had previously visited China in 2015 to attend the 'Understanding China' International Conference. This visit to China involves such a large number of Chinese-speaking representatives, indicating the hope that Sullivan can truly understand China during this trip.",
      "output": [
        {
          "category": "NewsEvent",
          "name": "Sullivan's First Visit to China During His Tenure",
          "properties": {
            "cause": null,
            "impact": null,
            "relation": "Visit to China",
            "context": "U.S. National Security Advisor Jake Sullivan arrived in Beijing by plane, beginning his first visit to China during his tenure",
            "subject": [
              {
                "name": "Jake Sullivan",
                "type": "Political Figure"
              }
            ],
            "time": {
              "name": "On the afternoon of August 27th, Beijing time"
            },
            "outcome": "First visit to China during his tenure",
            "location": {
              "name": "Beijing",
              "type": "City"
            },
            "process": null,
            "object": [
              {
                "name": "China",
                "type": "Country"
              }
            ]
          }
        },
        {
          "category": "NewsEvent",
          "name": "Sullivan Attended the 'Understanding China' International Conference in 2015",
          "properties": {
            "cause": null,
            "impact": null,
            "relation": "Attend Conference",
            "context": "Sullivan himself had previously visited China in 2015 to attend the 'Understanding China' International Conference",
            "subject": [
              {
                "name": "Jake Sullivan",
                "type": "Political Figure"
              }
            ],
            "time": {
              "name": "2015"
            },
            "outcome": null,
            "location": {
              "name": "China",
              "type": "Country"
            },
            "process": null,
            "object": [
              {
                "name": "'Understanding China' International Conference",
                "type": "Conference"
              }
            ]
          }
        }
      ]
    }
    ```
    
    As shown in the example, an event contains multiple attributes, and each attribute value aligns with the types defined in the schema.  
    The extraction of entity types is similar to that of event types, and complex entity types along with their attributes can also be defined following the above example.
    
    **Notes:**
    
    1. For the complete workflow of schema-constrained knowledge extraction, refer to the encyclopedia data QA example: `examples/baike`.
    2. The current version (v0.6) only supports the `MultiValue` attribute in the `constraints` field of SPG schema, while `NotNull` constraints are not yet supported.
    
    ### 2.2.1、Custom prompt
    Taking the `BaiKe` dataset as an example, the prompt  supporting the schema-constraint extraction  is located at `kag/builder/prompt/spg_prompt.py`. Its primary function is to parse the schema and automatically populate it into the template. Specifically, we have defined three schema-based prompts: `entity extraction`, `relation extraction`, and `event extraction`. Users can modify the examples within these prompts, along with their custom schema.
    
    ### 2.2.2、Prompt Application
    The KAG framework includes a built-in component called `SchemaConstraintExtractor`, registered under the name "`schema_constraint_extractor`". Users need to utilize this Extractor in conjunction with custom prompts to achieve schema-constrained knowledge extraction. Compared to the schema-free knowledge extraction pipeline, the `SchemaConstraintExtractor` adds event extraction capabilities, separating entity and event types for extraction based on the schema. The specific workflow is as follows:
    
    1. **Entity Extraction**: Using `ner_prompt`
    2. **Entity Standardization**: Using `std_prompt`
    3. **Triple Extraction**: Using `triple_prompt`
    4. **Event Extraction**: Using `event_prompt`
    5. **Aggregating Extracted data into a graph**
    
    ```python
    class SchemaConstraintExtractor(ExtractorABC):
        """
        Perform knowledge extraction for enforcing schema constraints, including entities, events and their edges.
        The types of entities and events, along with their respective attributes, are automatically inherited from the project's schema.
        """
    
        def __init__(
            self,
            llm: LLMClient,
            ner_prompt: PromptABC = None,
            std_prompt: PromptABC = None,
            relation_prompt: PromptABC = None,
            event_prompt: PromptABC = None,
            external_graph: ExternalGraphLoaderABC = None,
        ):
            """
            Initializes the SchemaBasedExtractor instance.
    
            Args:
                llm (LLMClient): The language model client used for extraction.
                ner_prompt (PromptABC, optional): The prompt for named entity recognition. Defaults to None.
                std_prompt (PromptABC, optional): The prompt for named entity standardization. Defaults to None.
                relation_prompt (PromptABC, optional): The prompt for relation extraction. Defaults to None.
                event_prompt (PromptABC, optional): The prompt for event extraction. Defaults to None.
                external_graph (ExternalGraphLoaderABC, optional): The external graph loader for additional data. Defaults to None.
            """
    
    ```
    
    
    
    # 3、Graph Reasoning Related Prompts
    The default prompts are located in `kag/solver/prompt/default/`
    
    ## 3.1、Planner Prompts
    ### 3.1.1、Prompt files
    The default prompt file path for the `planner` is as follows:
    
    ```bash
    kag.solver.prompt
                ├── default
                │   ├── logic_form_plan.py #default planner prompt
                ├── lawbench
                └── medical
    ```
    
    The prompts for the `planner` are divided into two parts:
    
    ```cypher
    template_en = f"""
    {{
        {instruct_en}
        {default_case_en}
        "output_format": "Only output words in answer, for examples: `Step`, `Action` content",
        "query": "$question"
    }}"""
    ```
    
    + instruct: Defines the basic information of the operator. DONOT modify this, otherwise the subsequent executor will not be able to parse it.
    
    ```cypher
    instruct_en = """    "instruction": "",
        "function_description": "functionName is operator name;the function format is functionName(arg_name1=arg_value1,[args_name2=arg_value2, args_name3=arg_value3]),括号中为参数，被[]包含的参数为可选参数，未被[]包含的为必选参数",
        "function": [
          {
              "functionName": "get_spo",
              "function_declaration": "get_spo(s=s_alias:entity_type[entity_name], p=p_alias:edge_type, o=o_alias:entity_type[entity_name])",
              "description": "Find SPO information. 's' represents the subject, 'o' represents the object, and they are denoted as variable_name:entity_type[entity_name]. The entity name is an optional parameter and should be provided when there is a specific entity to query. 'p' represents the predicate, which can be a relationship or attribute, denoted as variable_name:edge_type_or_attribute_type. Each variable is assigned a unique variable name, which is used for reference in subsequent mentions. Note that 's', 'p', and 'o' should not appear repeatedly within the same expression; only one set of SPO should be queried at a time. When a variable is a reference to a previously mentioned variable name, the variable name must match the previously mentioned variable name, and only the variable name needs to be provided; the entity type is only given when it is first introduced."
          },
          {
              "functionName": "count",
              "function_declaration": "count(alias)->count_alias",
              "description": "Count the number of nodes. The parameter should be a specified set of nodes to count, and it can only be variable names that appear in the get_spo query. The variable name 'count_alias' represents the counting result, which must be of int type, and this variable name can be used for reference in subsequent mentions."
          },
          {
              "functionName": "sum",
              "function_declaration": "sum(alias, num1, num2, ...)->sum_alias",
              "description": "Calculate the sum of data. The parameter should be a specified set to sum, which can be either numbers or variable names mentioned earlier, and its content must be of numeric type. The variable name 'sum_alias' represents the result of the calculation, which must be of numeric type, and this variable name can be used for reference in subsequent mentions."      },
          {
              "functionName": "sort",
              "function_declaration": "sort(set=alias, orderby=o_alias or count_alias or sum_alias, direction=min or max, limit=N)",
              "description": "Sort a set of nodes. The 'set' parameter specifies the set of nodes to be sorted and can only be variable names that appear in the get_spo query. The 'orderby' parameter specifies the basis for sorting, which can be the relationship or attribute name of the nodes. If it has been mentioned earlier, an alias should be used. The 'direction' parameter specifies the sorting order, which can only be 'min' (ascending) or 'max' (descending). The 'limit' parameter specifies the limit on the number of output results and must be of int type. The sorted result can be used as the final output."      },
          {
              "functionName": "compare",
              "function_declaration": "compare(set=[alias1, alias2, ...], op=min|max)",
              "description": "Compare nodes or numeric values. The 'set' parameter specifies the set of nodes or values to be compared, which can be variable names that appear in the get_spo query or constants. The 'op' parameter specifies the comparison operation: 'min' to find the smallest and 'max' to find the largest."
          },
          {
              "functionName": "get",
              "function_decl:aration": "get(alias)",
              "description": "Return the information represented by a specified alias. This can be an entity, a relationship path, or an attribute value obtained in the get_spo query. It can be used as the final output result."
          }
        ],"""
    ```
    
    + default_case：Provides few-shot examples for the LLM. This part is customizable.
    
    ```cypher
    default_case_en = """"cases": [
            {
                "query": "Which sports team for which Cristiano Ronaldo played in 2011 was founded last ?",
                "answer": "Step1:Which Sports Teams Cristiano Ronaldo Played for in 2011 ?\nAction1:get_spo(s=s1:Player[Cristiano Ronaldo],p=p1:PlayedForIn2011Year,o=o1:SportsTeam)\nStep2:In which year were these teams established ?\nAction2:get_spo(s=o1,p=p2:FoundationYear,o=o2:Year)\nStep3:Which team was founded last ?\nAction3:sort(set=o1, orderby=o2, direction=max, limit=1)"
            },
            {
                "query": "Who was the first president of the association which published Journal of Psychotherapy Integration?",
                "answer": "Step1:Which association that publishes the Journal of Psychotherapy Integration ?\nAction1:Journal(s=s1:Player[Psychotherapy Integration],p=p1:Publish,o=o1:Association)\nStep2:Who was the first president of that specific association?\nAction2:get_spo(s=o1,p=p2:FirstPresident,o=o2:Person)"
            },
            {
                "query": "When did the state where Pocahontas Mounds is located become part of the United States?",
                "answer": "Step1:Which State Where Pocahontas Mounds is Located ?\nAction1:get_spo(s=s1:HistoricalSite[Pocahontas Mounds], p=p1:LocatedIn, o=o1:State)\nStep2:When did this state become a part of the United States ？\nAction2:get_spo(s=o1, p=p2:YearOfBecamingPartofTheUnitedStates, o=o2:Date)"
            },
            {
                "query": "Which of the two tornado outbreaks killed the most people?",
                "answer": "Step1:Which is the first tornado outbreaks ?\nAction1:get_spo(s=s1:Event[Tornado Outbreak], p=p1:TheFirst, o=o1:Event)\nStep2:Which is the second tornado outbreaks ?\nAction2:get_spo(s=s2:Event[Tornado Outbreak], p=p2:TheSecond, o=o2:Event)\nStep3:How many people died in the first tornado outbreak ?\nAction3:get_spo(s=s1, p=p3:KilledPeopleNumber, o=o3:Number)\nStep4:How many people died in the second tornado outbreak ?\nAction4:get_spo(s=s2, p=p4:KilledPeopleNumber, o=o4:Number)\nStep5:To compare the death toll between two tornado outbreaks to determine which one had more fatalities.\nAction5:compare(set=[o3,o4], op=max)"
            }
        ],"""
    ```
    
    ### 3.1.2、Custom prompt
    Please refer to [**Section 2.1.2**](#LYO9w) for detailed steps.
    
    ### 3.1.3、Prompt Application
    Here is an example of a RiskMining application,  only `default_case_en` is rewrited.
    
    ```python
    import logging
    import re
    from string import Template
    from typing import List
    
    logger = logging.getLogger(__name__)
    
    from kag.interface import PromptABC
    
    
    @PromptABC.register("riskmining_lf_plan")
    class MyLogicFormPlanPrompt(LogicFormPlanPrompt):
        default_case_en = """"cases": [
            {
                "Action": "Is Zhang*San the developer of a gambling app?",
                "answer": "Step1: Query the classification of Zhang*San\nAction1: get_spo(s=s1:NaturalPerson[Zhang*San], p=p1:BelongsTo, o=o1:RiskUser)\nOutput: Output o1\nAction2: get(o1)"
            }
        ],"""
        def __init__(self, language: str = "", **kwargs):
            super().__init__(language, **kwargs)
    
    ```
    
    ## 3.2、Reasoner Prompt
    The default prompt files for the `reasoner` are as follows, please refer to [Section 3.1](#t39oM) for customization.
    
    ```bash
    kag.solver.prompt
                ├── default
                │   ├── deduce_choice.py # Reasoning prompt for single-choice questions
                │   ├── deduce_entail.py # Reasoning prompt for entailment inference
                │   ├── deduce_judge.py # Reasoning prompt for boolean inference
                │   ├── deduce_multi_choice.py # Reasoning prompt for multiple-choice questions
                │   ├── question_ner.py # NER recognition for sub-questions
                │   ├── solve_question.py  # Generate sub-question answers based on retrieved documents, graph relationships, and historical records
                │   ├── solve_question_without_docs.py # Generate sub-question answers based on retrieved graph relationships and historical records
                │   ├── solve_question_without_spo.py # Generate sub-question answers based on retrieved documents and historical records
                │   └── spo_retrieval.py   # Select qualified SPO tuples
                ├── lawbench
                └── medical
    ```
    
    ## 3.3、Reflector Prompt
    The default prompt file paths for the `reflector` are as follows, please refer to [Section 3.1](#t39oM) for customization.
    
    ```bash
    kag.solver.prompt
                ├── default
                │   ├── resp_extractor.py # Extract government-related information based on existing data
                │   ├── resp_judge.py # Determine whether the sub-question can be answered based on current information
                │   ├── resp_reflector.py # When resp_judge returns false, this prompt can generate a new question
                │   ├── resp_verifier.py # Verify that the evidence extracted by the extractor aligns with the question's context
                ├── lawbench
                └── medical
    ```
    
    ## 3.4、Generator Prompt
    The default prompt file paths for the `generator` are as follows, please refer to [Section 3.1](#t39oM) for customization.
    
    ```bash
    kag.solver.prompt
                ├── default
                │   ├── resp_generator.py
                ├── lawbench
                └── medical
    ```
    
风险细节：
    敏感类型：URL地址(url)
    敏感细节：{hitWord=https://openspg.yuque.com/ndx6g9/cwh47i/zwxvp3n4xog1vxsv)}
风险内容位置：
    CommitId: 2911f9fb9d07e718b153121318ef12824a30a299
    文件名：website/docs/en/UserGuide/KnowledgeBaseConfiguration.md
    文件行：1~293
风险内容：
    ---
    sidebar_position: 3
    ---
    
    # KnowledgeBase Configuration
    
    # 1、KnowledgeBase Configuration
    ## 1.1、Global Configuration
    When you enter the page for the first time, you will be prompted to create a default global configuration when creating a knowledge base, which mainly includes general configuration (image storage configuration, vector configuration, prompt word configuration), model configuration, and user configuration. After the global configuration is successfully set up, the subsequent projects created can use it directly.
    
    ![1736317066886-7a696302-06e7-4fdc-8cd3-df07f88818e4.png](./img/MzJLAuIV400Scg5c/1736317066886-7a696302-06e7-4fdc-8cd3-df07f88818e4-011816.png)
    
    + **General Configuration**
    
    Global settings for image storage configuration, vector configuration, and prompt word configuration can be performed
    
    ![1736162669141-77a4f464-a878-42d8-9b73-8c6bb5dc3cf1.png](./img/MzJLAuIV400Scg5c/1736162669141-77a4f464-a878-42d8-9b73-8c6bb5dc3cf1-286244.png)
    
    + **Model configuration**
    
    ![1736162682327-0f108e92-bdec-48ad-8a59-97ad8491722b.png](./img/MzJLAuIV400Scg5c/1736162682327-0f108e92-bdec-48ad-8a59-97ad8491722b-346621.png)
    
    ## 1.2、Create knowledge base
    ![1736317683162-6dcec56e-ea10-435c-8bcf-9c1b9ef5be77.jpeg](./img/MzJLAuIV400Scg5c/1736317683162-6dcec56e-ea10-435c-8bcf-9c1b9ef5be77-373607.jpeg)
    
    ### 1.2.1、Basic Configuration
    + Chinese name of knowledge base
    
    Required. The Chinese name of the knowledge base, used for page display
    
    + English name of knowledge base
    
    Required. The English name of the knowledge base must start with a capital letter and can only be a combination of letters and numbers, with a minimum of 3 characters. Used for schema prefixes and graph storage data isolation
    
    + ****Figure Storage Configuration
    
    | **Parameter name** | **Parameter Description** |
    | --- | --- |
    | database | The database name of the graph storage is fixed and consistent with the English name of the knowledge base and cannot be modified |
    | password | The default value of the built-in openspg-neo4j is: neo4j@openspg |
    | uri | The default value of the built-in openspg-neo4j is: neo4j://release-openspg-neo4j:7687 |
    | user | The default value of the built-in openspg-neo4j is: neo4j |
    
    
    + Vector configuration
    
    Provides vector generation service, supports bge and openai-embedding. It is recommended to use bge-m3 for English and bge-base-zh for Chinese. For details, refer to Embedding Model Configuration.
    
    Example of configuring a business presentation model service such as Silicon Mobility:
    
    ```json
    {
        "type": "openai",
        "model": "BAAI/bge-m3",
        "base_url": "https://api.siliconflow.cn/v1",
        "api_key": "YOUR_API_KEY",
        "vector_dimensions": "1024"
    }
    ```
    
    + **Model service availability testing**
    
    When the configuration is saved, kag will call the large model API according to the representation model configuration. If the call fails, it will prompt that the save failed. Users can use the curl command in the openspg container to verify the service accessibility and whether the api-key has expired.
    
    ```bash
    $ curl --request POST \
      --url https://api.siliconflow.cn/v1/embeddings \
      --header 'Authorization: Bearer <token>' \
      --header 'Content-Type: application/json' \
      --data '{
        "model": "BAAI/bge-large-zh-v1.5",
        "input": "Silicon-based mobile embedding is now available. Come and try it out!",
        "encoding_format": "float"
      }'
    ```
    
    ****
    
    + Prompt words in Chinese and English
    
    Used to determine whether to use Chinese (zh) or English (en) when calling the model. Example:
    
    ```json
    {
      "biz_scene":"default",
      "language":"zh"
    }
    ```
    
    ### 1.2.2、Model configuration
    ![1736162949982-22938e31-bb8a-46ff-bfe2-12f556f35d29.png](./img/MzJLAuIV400Scg5c/1736162949982-22938e31-bb8a-46ff-bfe2-12f556f35d29-472001.png)
    
    Used for model construction and question-answering. Supports Open-AI compatible APIs (ChatGPT3.5, DeepSeek, Qwen2, etc.), provides MAAS, VLLM, Ollama and other modes, for details, refer to Generate (Chat) Model Configuration.
    
    + **Example of configuring commercial model generation services such as deepseek:**
    
    ```json
    {
      "type": "maas",
      "base_url": "https://api.deepseek.com",
      "api_key": "deepseek api key",
      "model": "deepseek-chat"
    }
    ```
    
    + **Model service availability testing**
    
    When the configuration is saved, kag will call the large model api according to the generated model configuration. If the call fails, it will prompt that the save failed. Users can use the curl command in the openspg container to verify the service accessibility and whether the api-key has expired.
    
    ```bash
    
    $ curl https://api.deepseek.com/chat/completions \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer <DeepSeek API Key>" \
      -d '{
            "model": "deepseek-chat",
            "messages": [
              {"role": "system", "content": "You are a helpful assistant."},
              {"role": "user", "content": "Hello!"}
            ],
            "stream": false
          }'
    ```
    
    # 2、Schema management
    Knowledge Base Management - Knowledge Model, enter the knowledge base schema management page
    
    ## 2.1、Editing the schema
    When the knowledge base is created, some schemas are built in
    
    ![1736163077300-72b6711d-08d7-444c-88dc-05a55be78549.png](./img/MzJLAuIV400Scg5c/1736163077300-72b6711d-08d7-444c-88dc-05a55be78549-297884.png)
    
    
    
    ```yaml
    namespace TCDemo1
    
    NaturalScience(自然科学): EntityType
    	properties:
    		semanticType(semanticType): Text
    			index:Text
    		desc(desc): Text
    			index:TextAndVector
    
    Concept(概念): EntityType
    	properties:
    		semanticType(semanticType): Text
    			index:Text
    		desc(desc): Text
    			index:TextAndVector
    
    Building(建筑): EntityType
    	properties:
    		semanticType(semanticType): Text
    			index:Text
    		creator(创作者): Person
    		desc(desc): Text
    			index:TextAndVector
    
    Person(人物): EntityType
    	properties:
    		desc(desc): Text
    			index:TextAndVector
    		semanticType(semanticType): Text
    			index:Text
    
    Astronomy(天文学): EntityType
    	properties:
    		desc(desc): Text
    			index:TextAndVector
    		semanticType(semanticType): Text
    			index:Text
    
    Chunk(文本块): EntityType
    	properties:
    		content(content): Text
    			index:TextAndVector
    
    Event(事件): EntityType
    	properties:
    		semanticType(semanticType): Text
    			index:Text
    		desc(desc): Text
    			index:TextAndVector
    
    Keyword(关键词): EntityType
    	properties:
    		semanticType(semanticType): Text
    			index:Text
    		desc(desc): Text
    			index:TextAndVector
    
    Transport(运输): EntityType
    	properties:
    		semanticType(semanticType): Text
    			index:Text
    		desc(desc): Text
    			index:TextAndVector
    
    Date(日期): EntityType
    	properties:
    		desc(desc): Text
    			index:TextAndVector
    		semanticType(semanticType): Text
    			index:Text
    
    GeographicLocation(地理位置): EntityType
    	properties:
    		desc(desc): Text
    			index:TextAndVector
    		semanticType(semanticType): Text
    			index:Text
    
    Organization(组织机构): EntityType
    	properties:
    		desc(desc): Text
    			index:TextAndVector
    		semanticType(semanticType): Text
    			index:Text
    
    ArtificialObject(人造物体): EntityType
    	properties:
    		semanticType(semanticType): Text
    			index:Text
    		desc(desc): Text
    			index:TextAndVector
    
    Medicine(药物): EntityType
    	properties:
    		semanticType(semanticType): Text
    			index:Text
    		desc(desc): Text
    			index:TextAndVector
    
    Works(作品): EntityType
    	properties:
    		desc(desc): Text
    			index:TextAndVector
    		semanticType(semanticType): Text
    			index:Text
    
    Others(其他): EntityType
    	properties:
    		semanticType(semanticType): Text
    			index:Text
    		desc(desc): Text
    			index:TextAndVector
    
    Creature(生物): EntityType
    	properties:
    		semanticType(semanticType): Text
    			index:Text
    		desc(desc): Text
    			index:TextAndVector
    
    SemanticConcept(语义概念): EntityType
    	properties:
    		semanticType(semanticType): Text
    			index:Text
    		desc(desc): Text
    			index:TextAndVector
    ```
    
    + The knowledge base schema currently only provides one operation entry, and the operation mode is declarative schema.
    + Schema editing content supports:
        - Create new entity types and meta-concept types
        - Create new relation types: entity-to-entity, entity-to-meta-concept, entity-to-standard type
        - Modify entity types, meta-concept types, and relationship types to modify the Chinese and English names of attributes
        - Add/delete attributes and rules for entity types and relationship types
        - Delete entity types, meta-concept types, and relationship types
        - Add, modify, and delete constraints, indexes, and rules for attributes
    
    ## 2.2、Dot pattern
    + The schema is displayed on the canvas in the form of points and edges
    + You can quickly locate points or relationships based on type or relationship
    + The canvas tool in the lower left corner can be used to adjust and download the canvas
    
    ![1736217087085-12b4bff4-3686-404d-af53-521c25955faf.png](./img/MzJLAuIV400Scg5c/1736217087085-12b4bff4-3686-404d-af53-521c25955faf-538764.png)
    
    ## 2.3、Tree Mode
    + Display the schema in a treeGraph and show the hierarchical relationship of the schema.
    
    ![1736216871164-8fb81a7b-7362-48ea-b6c1-9b588d022e67.png](./img/MzJLAuIV400Scg5c/1736216871164-8fb81a7b-7362-48ea-b6c1-9b588d022e67-352996.png)
    
    ## 2.4 Schema Details
    ### 2.4.1、Entity Type Details
    + Click an EntityType to view its details, including description, attributes, relations, and entity sampling data.
    
    ![1736163211480-752d8a07-bba4-4038-b70f-7ea7168e7f28.png](./img/MzJLAuIV400Scg5c/1736163211480-752d8a07-bba4-4038-b70f-7ea7168e7f28-193944.png)
    
    ### 2.4.2、RelationType Details
    Click a RelationType to view the details, including description, attributes, StartEntityType, EndEntityType.  
    
风险细节：
    敏感类型：URL地址(url)
    敏感细节：{hitWord=https://api.siliconflow.cn/v1}
{hitWord=https://api.siliconflow.cn/v1/embeddings}
{hitWord=https://api.deepseek.com}
{hitWord=https://api.deepseek.com/chat/completions}
风险内容位置：
    CommitId: 2911f9fb9d07e718b153121318ef12824a30a299
    文件名：website/docs/en/UserGuide/OpenSPGCompilation&Deployment.md
    文件行：1~162
风险内容：
    ---
    sidebar_position: 10
    ---
    
    # OpenSPG Compilation & Deployment
    
    # 1、Compilation Process
    ## 1.1、Pre-dependency 
    + **Java:**
    
    java 18 (recommend OpenJDK version 18) 
    
    + **Maven**:
    
    maven version 3.8 (recommend Maven version 3.8.5) 
    
    + **Scala:**
    
    idea Installs the scala plug-in and sets the scala directory under the reason to Sources Root. 
    
    ![1733976044587-9b9fc7fa-84d8-4c4f-b0f5-b0bf81fefe8c.png](./img/lay9zoTz5PVq87wH/1733976044587-9b9fc7fa-84d8-4c4f-b0f5-b0bf81fefe8c-688264.png)
    
    ![1733976020586-6cb64942-1663-41ba-ab85-7f69d04dcbf5.png](./img/lay9zoTz5PVq87wH/1733976020586-6cb64942-1663-41ba-ab85-7f69d04dcbf5-940027.png)
    
    + **Lombok:**
    
    idea Install the Lombok plugin 
    
    ![1733976051525-3378d1a7-34c5-4451-9327-f3912a286dc3.png](./img/lay9zoTz5PVq87wH/1733976051525-3378d1a7-34c5-4451-9327-f3912a286dc3-093684.png)
    
    + **Set Incrementality Type to IDEA**
    
    Settings > Build, Execution, Deployment > Compiler > Scala Compiler Set Incrementality Type to IDEA
    
    ![1739240988719-4b7fae23-72c7-43d2-a8c2-78efe005f65d.png](./img/lay9zoTz5PVq87wH/1739240988719-4b7fae23-72c7-43d2-a8c2-78efe005f65d-887268.png)
    
    ## 1.2、Source code download 
    clone the OpenSPG source code and open it in the IDE 
    
    ```bash
    git clone git@github.com:OpenSPG/openspg.git
    ```
    
    **subsequent commands must be executed in the root directory of the openspg code library. **
    
    ## 1.3、Source code compilation 
    execute the mvn compilation Command 
    
    ```bash
    mvn clean install -Dmaven.test.skip=true -Dspotless.check.skip -Dspotless.apply.skip
    ```
    
    ## 1.4、Start Mirroring
    Start MySQL, Graph, and other containers locally
    
    ```bash
    sh dev/test/docker-compose.sh
    ```
    
    ## 1.5、Start the server 
    the startup server Portal is located
    
    ```bash
    com.antgroup.openspg.server.arks.sofaboot.Application
    ```
    
    **At present, the front-end code is not open source. If you need to use http:// 127.0.0.1:8887 to operate the front-end visualization page, please install it through mirror image.**
    
    ****
    
    # **2、**Compilation Example(Mac)
    **Environment： MacBook Air **
    
    **Chip： Apple M1**
    
    ## **2.1、Install Homebrew**
    **After installing Homebrew, it is used to install git, wget, java and other tools. If it has already been installed, you can ignore it directly**
    
    ```shell
    ## Install Homebrew
    /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
    ## Edit .zshrc
    vim ~/.zshrc
    ## Add the following line
    export PATH="/opt/homebrew/bin:/opt/homebrew/sbin:$PATH"
    ## Make the changes take effect
    source ~/.zshrc
    ##Verify that Homebrew
    brew --version
    brew update
    ```
    
    [此处为语雀卡片，点击链接查看](https://www.yuque.com/ndx6g9/docs_en/mudrfu5ibk4eh26g#UCqa2)
    
    
    
    ## **2.2、Install git and wget**
    ```shell
    brew install git
    brew install wget
    git --version
    wget --version
    ```
    
    [此处为语雀卡片，点击链接查看](https://www.yuque.com/ndx6g9/docs_en/mudrfu5ibk4eh26g#rzpsS)
    
    
    
    ## **2.3、Install Maven And JAVA**
    ```shell
    cd /Users/opt
    ## Install maven 3.8.5
    wget https://alipay-sdtag.oss-cn-qingdao.aliyuncs.com/openspg/apache-maven-3.8.5.zip
    ## unzip
    unzip apache-maven-3.8.5.zip
    ## Install JAVA 18 （Other environment versions can be downloaded manually from https://jdk.java.net/archive/）
    wget https://download.java.net/java/GA/jdk18.0.2/f6ad4b4450fd4d298113270ec84f30ee/9/GPL/openjdk-18.0.2_macos-aarch64_bin.tar.gz
    ## unzip
    tar -xzvf openjdk-18.0.2_macos-aarch64_bin.tar.gz
    
    ## Setting Environment Variables
    vim ~/.zshrc
    ## Add java and maven addresses to PATH. Note: /Users/opt needs to be changed to your local installation path.
    export PATH="/Users/opt/jdk-18.0.2.jdk/Contents/Home/bin:/Users/opt/apache-maven-3.8.5/bin:/opt/homebrew/bin:/opt/homebrew/sbin:$PATH"
    ## Make the changes take effect
    source ~/.zshrc
    ## Verify
    java -version
    mvn -v
    ```
    
    [此处为语雀卡片，点击链接查看](https://www.yuque.com/ndx6g9/docs_en/mudrfu5ibk4eh26g#pXUIC)
    
    
    
    ## **2.4、Download OpenSPG source code, compile and start**
    ```shell
    cd /Users/opt
    
    ## Download openspg source code
    git clone --depth=1 https://github.com/OpenSPG/openspg.git
    
    ## 1.Open OpenSPG through IDEA
    ## 2.Install Lombok and Scala plugins
    ## 3.Set Incrementality Type to IDEA
    ##  Settings > Build, Execution, Deployment > Compiler > Scala Compiler Set Incrementality Type: Zinc -> IDEA
    
    ## Compiling the code
    mvn clean install -Dmaven.test.skip=true -Dspotless.check.skip -Dspotless.apply.skip
    
    ## Start Docker Docker. Desktop installation address(https://www.docker.com/products/docker-desktop/)
    
    ## Install mysql, neo4j image dependencies
    sh dev/test/docker-compose.sh
    
    ##Start the application
    com.antgroup.openspg.server.arks.sofaboot.Application
    
    ```
    
    [此处为语雀卡片，点击链接查看](https://www.yuque.com/ndx6g9/docs_en/mudrfu5ibk4eh26g#AmMes)
    
风险细节：
    敏感类型：URL地址(url)
    敏感细节：{hitWord=https://www.yuque.com/ndx6g9/docs_en/mudrfu5ibk4eh26g#UCqa2)}
{hitWord=https://www.yuque.com/ndx6g9/docs_en/mudrfu5ibk4eh26g#rzpsS)}
{hitWord=https://alipay-sdtag.oss-cn-qingdao.aliyuncs.com/openspg/apache-maven-3.8.5.zip}
{hitWord=https://jdk.java.net/archive/}
{hitWord=https://download.java.net/java/GA/jdk18.0.2/f6ad4b4450fd4d298113270ec84f30ee/9/GPL/openjdk-18.0.2_macos-aarch64_bin.tar.gz}
{hitWord=https://www.yuque.com/ndx6g9/docs_en/mudrfu5ibk4eh26g#pXUIC)}
{hitWord=https://www.yuque.com/ndx6g9/docs_en/mudrfu5ibk4eh26g#AmMes)}
风险内容位置：
    CommitId: 2911f9fb9d07e718b153121318ef12824a30a299
    文件名：website/docs/en/UserGuide/ModelConfiguration/RepresentationEmbeddingModel.md
    文件行：1~189
风险内容：
    ---
    sidebar_position: 2
    ---
    
    # Representation (embedding) model
    
    KAG dependency represents the model service, which is used to generate entity attributes and query embedding vectors during Graph construction and inference Q &amp; A. For the vector settings of entity attributes, see the relevant chapter of "knowledge modeling.   
    kag supports the representation model services of interfaces compatible with the openai class, such as openai and silicon-based flow. It also supports the representation model prediction services provided by ollama and xinference. At the same time, in the product mode, kag has built-in BAAI/bge-base-zh-v1.5 in the openspg-server container. The user can choose one of these ways to use.   
    **Special attention: ****embedding ****vectors generated by different representation models cannot be mixed even if they have the same dimensions; Therefore, in the knowledge base configuration, the configuration related to the representation model cannot be modified once it is set. **
    
    # Commercial** Model as a Service (MaaS)**
    Developers can go there themselves. [Silicon-based flow official website](https://docs.siliconflow.cn/api-reference/embeddings/create-embeddings), [openai official website](https://chat.openai.com/), complete the account registration and model service opening in advance, and obtain the api-key and fill it in the subsequent project configuration. 
    
    ## **Product Mode Configuration **
    in the product mode, the configuration is filled in the json string format.
    
    #### configuration Item 
    ![1736322619320-604c6495-4594-472a-a582-02e42c9be20e.png](./img/PkZm-n2tERjAQGGK/1736322619320-604c6495-4594-472a-a582-02e42c9be20e-540279.png)
    
    #### configuration Example
    ```bash
    # replace base_url with siliconflow hostaddr
    {
      "type": "openai",
      "model": "BAAI/bge-m3",
      "base_url": "https://api.siliconflow.cn/v1",
      "api_key": "YOUR_API_KEY",
      "vector_dimensions": "1024"
    }
    ```
    
    ```bash
    # base_url use openai default hostaddr
    {
      "type": "openai",
      "model": "text-embedding-ada-002",
      "base_url": "https://api.openai.com/v1",
      "api_key": "YOUR_API_KEY",
      "vector_dimensions": "1536"
    }
    ```
    
    | **Parameter Name ** | **parameter Description ** |
    | --- | --- |
    | vectorizer | specifies the class name of the KAG Vectorizer. OpenAIVectorizer supports vectorized model services compatible with OpenAI and OpenAI API.  |
    | model | visit [Models - OpenAI API ](https://platform.openai.com/docs/models/embeddings)view the available OpenAI presentation models, such as text-embedding-ada-002, text-embedded -3-small, and so on.    Visit [Embedding Models - siliconflow API ](https://docs.siliconflow.cn/api-reference/embeddings/create-embeddings)view the available presentation models, such as BAAI/bge-m3, etc.  |
    | api_key | to get your api_key, go [silicon-based flow official website](https://docs.siliconflow.cn/api-reference/embeddings/create-embeddings), [openai official website](https://chat.openai.com/)get. |
    
    
    #### model Service Availability Test
    When the configuration is saved, kag calls the big model api based on the representation model configuration. If the call fails, it prompts that the save fails. You can use the curl Command in the openspg container to verify the reachability of the service and whether the api-key has expired.
    
    ```bash
    # replace <token> with api-key acquired from model-api
    
    $ curl --request POST \
      --url https://api.siliconflow.cn/v1/embeddings \
      --header 'Authorization: Bearer <token>' \
      --header 'Content-Type: application/json' \
      --data '{
          "model": "BAAI/bge-m3",
          "input": "硅基流动embedding上线，多快好省的 embedding 服务，快来试试吧"
        }'
    ```
    
    ## **Developer Mode**
    you can change the representation model of all aspects of kag based on the  configuration file kag_config.yaml.
    
    ```bash
    vectorize_model: &vectorize_model
      api_key: key
      base_url: https://api.siliconflow.cn/v1/
      model: BAAI/bge-m3
      type: openai
    ```
    
    Note: Vector models cannot be mixed. If the model configuration needs to be modified, it is recommended to create a new knowledge base. 
    
    # **Local Model Service (ollama) **
    Indicates the local deployment of the model, which has a large resource overhead and workload. Only one example is provided here. If the developer is not clear about the specific details, we recommend that you purchase the business model api.
    
    ## **Model service startup**
    ### **ollama model reasoning service **
    + **Installing ollama **
        - **mac User: **brew install ollama 
        - **windows and linux users: **to [ollama official website](https://ollama.com/)download ollama 
    + **start the model service: **
        - **start ollama:**
    
    ```bash
    # listening all requests
    $ export OLLAMA_HOST=0.0.0.0:11434
    $ ollama serve
    ```
    
        - **pull model: **
    
    ```bash
    $ ollama pull bge-m3
    ```
    
        - **test:**
    
    ```bash
    # service testing
    
    $ curl http://127.0.0.1:11434/v1/embeddings -d '
        {
        "model": "bge-m3",
        "input": [
            "work"
        ]
    }'
    ```
    
    ## **Product Mode Configuration **
    ### **Configuration Items & Examples **
    In the product mode, the configuration is filled in the json string format. 
    
    + **configuration Item**
    
    ![1736322653035-207ca81e-89d9-42cf-bc71-42d2e4c60592.png](./img/PkZm-n2tERjAQGGK/1736322653035-207ca81e-89d9-42cf-bc71-42d2e4c60592-743735.png)
    
    + **configuration Example**
    
    ```bash
    # replace base_url with real hostaddr
    # OpenAIVectorizer will append postfix of "/embeddings", there is no need for user to provide url
    
    vectorize_model: &vectorize_model
      api_key: EMPTY
      base_url: http://host.docker.internal:11434/v1
      model: bge-m3
      type: openai
      vector_dimensions: 1024
    
    ```
    
    Note: Vector models cannot be mixed. If the model configuration needs to be modified, it is recommended to create a new knowledge base.
    
    ### **Representation model domain name settings **
    when a user runs kag in the product mode, it involves the request of accessing the model service outside the container. For the following situations: 
    
    + **the kag container and the model inference service are deployed on the same host: **
        - **Mac & Windows Environment: **access to host.docker.int ernal in the container can be routed to the service on the host, and modify the domain name corresponding to base_url in the generated model configuration.
    
    ```bash
    # bge-m3 can be accessed in container through: 
    
    $ curl http://host.docker.internal:11434/v1/embeddings -d '
        {
        "model": "bge-m3",
        "input": [
            "work"
        ]
    }'
    ```
    
        - **Linux environment: **to access 172.17.0.1 in the container, you can access the host by accessing the gateway of the docker0 network, and modify the domain name corresponding to base_url in the generation model configuration.
    
    ```bash
    # bge-m3 can be accessed in container through: 
    
    $ curl http://172.17.0.1:11434/v1/embeddings -d '
        {
        "model": "bge-m3",
        "input": [
            "work"
        ]
    }'
    ```
    
    + **kag container and model inference service are deployed on different hosts: **
    
    request Routing is implemented by accessing the ip address of the host where the model service is located. 
    
    ## **Developer mode configuration**
    You can change the representation model of all aspects of kag based on the configuration file kag_config.yaml.
    
    ```bash
    vectorize_model: &vectorize_model
      api_key: key
      base_url: https://api.siliconflow.cn/v1/
      model: BAAI/bge-m3
      type: openai
      vector_dimensions: 1024
    ```
    
    Note: Vector models cannot be mixed. If the model configuration needs to be modified, it is recommended to create a new knowledge base.  
风险细节：
    敏感类型：URL地址(url)
    敏感细节：{hitWord=https://docs.siliconflow.cn/api-reference/embeddings/create-embeddings)}
{hitWord=https://chat.openai.com/)}
{hitWord=https://api.siliconflow.cn/v1}
{hitWord=https://api.openai.com/v1}
{hitWord=https://platform.openai.com/docs/models/embeddings)view}
{hitWord=https://docs.siliconflow.cn/api-reference/embeddings/create-embeddings)view}
{hitWord=https://docs.siliconflow.cn/api-reference/embeddings/create-embeddings)}
{hitWord=https://chat.openai.com/)get.}
{hitWord=https://api.siliconflow.cn/v1/embeddings}
{hitWord=https://api.siliconflow.cn/v1/}
{hitWord=https://ollama.com/)download}
{hitWord=http://host.docker.internal:11434/v1}
{hitWord=http://host.docker.internal:11434/v1/embeddings}
{hitWord=https://api.siliconflow.cn/v1/}
风险内容位置：
    CommitId: 2911f9fb9d07e718b153121318ef12824a30a299
    文件名：website/docs/en/UserGuide/ModelConfiguration/GenerativeChatModelConfiguration.md
    文件行：1~306
风险内容：
    ---
    sidebar_position: 1
    ---
    
    # Generative (chat) model configuration
    
    when configuring the model generation service for kag, you can select business model apis or local model services based on vllm, ollama, and xinference. 
    
    # 1. Commercial Model As A Service (MaaS) 
    maas stands for (Model As Service). kag supports all online big model services that are compatible with openai, such as deepseek, qwen, and openai.   
    Developers can go there themselves. [deepseek official website](https://www.deepseek.com/), [tongyi Thousand Questions official website](https://tongyi.aliyun.com/), [openai official website](https://chat.openai.com/), complete the account registration and model service opening in advance, and obtain the api-key and fill it in the subsequent project configuration. 
    
    ## 1.1** Product Mode Configuration **
    in the product mode, the configuration is filled in the json string format. 
    
    #### configuration Item 
    ![1736322694583-d57ebcb8-8048-4aad-9548-c605806640bb.png](./img/5QwPnXeg23q0GNnJ/1736322694583-d57ebcb8-8048-4aad-9548-c605806640bb-130107.png)
    
        - **maas**
    
    ```json
    {
      "model": "deepseek-chat",
      "base_url": "https://api.deepseek.com",
      "api_key": "deepseek api key"
    }
    ```
    
    #### Model Service Accessibility Test 
    when the configuration is saving, KAG will first call the generative model api to test accessibility. You can use the curl Command in the openspg container to verify the accessibility of the service and whether the api-key has expired.
    
    ```bash
    # replace <DeepSeek API Key> with your api-key
    
    $ curl https://api.deepseek.com/chat/completions \
      -H "Content-Type: application/json" \
      -H "Authorization: Bearer <DeepSeek API Key>" \
      -d '{
            "model": "deepseek-chat",
            "messages": [
              {"role": "system", "content": "You are a helpful assistant."},
              {"role": "user", "content": "Hello!"}
            ],
            "stream": false
          }'
    ```
    
    ## 1.2 **Developer Mode **
    you can change the large model that all aspects of kag depend on according to the the configuration file kag_config.yaml. User can define different LLM model and refer it in different stages of KAG-Builder or KAG-Solver.
    
    ```yaml
    # gpt-3.5-turbo
    openie_llm: &openie_llm
      api_key: your openai api_key
      base_url: https://api.openai.com
      model: gpt-3.5-turbo
      type: maas
    
    chat_llm: &chat_llm
      api_key: key
      base_url: https://api.openai.com
      model: gpt-3.5-turbo
      type: maas
    ```
    
    ```bash
    # qwen
    openie_llm: &openie_llm
      api_key: put your tongyi api key here
      base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
      model: qwen-plus
      type: maas
    
    chat_llm: &chat_llm
      api_key: put your tongyi api key here
      base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
      model: qwen-plus
      type: maas
    ```
    
    ```bash
    # Deepseek
    openie_llm: &openie_llm
      api_key: put your deepseek api key here
      base_url: https://api.deepseek.com
      model: deepseek-chat
      type: maas
    
    chat_llm: &chat_llm
      api_key: put your deepseek api key here
      base_url: https://api.deepseek.com
      model: deepseek-chat
      type: maas
    ```
    
    Note: In developer mode, after the configuration is modified, it needs to be synchronized to the server. Specific reference documents: [update Project Configuration](https://openspg.yuque.com/ndx6g9/cwh47i/rs7gr8g4s538b1n7#vmE7s).
    
    # 2 **Local model service (vllm/ollama)**
    ## 2.1 **Model service startup**
    kag supports the docking of openai large model inference services. You can choose one of the model service frameworks, such as ollama, vllm, and xinference, to build your own large Model Inference Service. 
    
    ### 2.1.1 **ollama Model Inference Service**
    + **installing ollama **
        - **mac User: **brew install ollama
        - **windows and linux users: **to [ollama official website](https://ollama.com/)download ollama 
    + **start the model service: **
        - **start ollama:**
    
    ```bash
    # listening for all requests
    $ export OLLAMA_HOST=0.0.0.0:11434
    $ ollama serve
    ```
    
        - **pull model:**
    
    ```bash
    $ ollama pull qwen2.5:3b
    ```
    
    You can start the model service 
    
        - **test:**
    
    ```bash
    # 发送测试请求
    $ curl http://localhost:11434/v1/chat/completions \
        -H "Content-Type: application/json" \
        -d '{
            "model": "qwen2.5:3b",
            "messages": [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "你是谁？"}
            ]
        }'
    
    $ {"id":"chatcmpl-360","object":"chat.completion","created":1732100271,"model":"qwen2.5:3b","system_fingerprint":"fp_ollama","choices":[{"index":0,"message":{"role":"assistant","content":"我是来自阿里云的大规模语言模型，我叫通义千问。很乐意为您提供帮助，请您告诉我您具体的问题或需求是什么，期待与您的交流合作！"},"finish_reason":"stop"}],"usage":{"prompt_tokens":22,"completion_tokens":37,"total_tokens":59}}
    ```
    
    ### 2.1.2 **vllm Model Inference Service**
    According to the official vLLM blog [vLLM: Easy, Fast, and Cheap LLM Serving with PagedAttention](https://blog.vllm.ai/2023/06/20/vllm.html)said: two experiments of reasoning LLaMA-7 B on NVIDIA A10 GPU and reasoning LLaMA-13 B on NVIDIA A100 GPU(40GB) were conducted. **In terms of throughput, vLLM is 24 times higher than the most basic HuggingFace Transformers and 3.5 times higher than TGI. **.
    
    + first of all, prepare a GPU environment, you can refer to this article: [GPU environment construction guide: how to use GPU in bare metal, Docker, K8s and other environments](https://www.lixueduan.com/posts/ai/01-how-to-use-gpu/)
    + to avoid interference, we recommend that you use conda to create a separate Python virtual environment to install vLLM.
    
    ```bash
    $ conda create -n vllm_py310 python=3.10
    
    $ conda activate vllm_py310
    
    $ pip install vllm
    ```
    
    + **model download, you can download models through modelsope**
    
    ```bash
    $ pip install modelscope
    $ modelscope download --model Qwen/Qwen2.5-7B-Chat
    ```
    
    + **start the vllm service**
    
    ```bash
    modelpath=/path/to/model/Qwen2.5-7B-Chat
    
    # 
    python3 -m vllm.entrypoints.openai.api_server \
            --model $modelpath \
            --served-model-name qwen25-7B-chat \
            --trust-remote-code
    
    ```
    
    + **send Test Request**
    
    ```bash
    # replace model with predefined ${served-model-name} 
    curl http://localhost:8000/v1/chat/completions \
        -H "Content-Type: application/json" \
        -d '{
            "model": "qwen25-7B-chat",
            "messages": [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "你是谁？"}
            ]
        }'
    
    ```
    
    ### 2.1.3 **Container Access Host Model Service**
    When a user runs kag in the product mode, it involves the request of accessing the model service outside the container. For the following situations: 
    
    + **the kag container and the model inference service are deployed on the same host:**
        - **Mac & Windows Environment: **access to host.docker.int ernal in the container can be routed to the service on the host, and modify the domain name corresponding to base_url in the generated model configuration.
    
    ```bash
    # qwen2.5:3b within ollama can be accessied through:
    $ curl http://host.docker.internal:11434/v1/chat/completions \
        -H "Content-Type: application/json" \
        -d '{
            "model": "qwen2.5:3b",
            "messages": [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "who are you？"}
            ]
        }'
    ```
    
        - **Linux environment: **to access 172.17.0.1 in the container, you can access the host by accessing the gateway of the docker0 network, and modify the domain name corresponding to base_url in the generation model configuration.
    
    ```bash
    # qwen2.5:3b within ollama can be accessied through:
    $ curl http://172.17.0.1:11434/v1/chat/completions \
        -H "Content-Type: application/json" \
        -d '{
            "model": "qwen2.5:3b",
            "messages": [
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": "你是谁？"}
            ]
        }'
    ```
    
    + **kag container and model inference service are deployed on different hosts:**
    
    Request Routing is implemented by accessing the ip address of the host where the model service is located. 
    
    ## 2.2 **Model service configuration **
    ### 2.2.1 **Product Mode Configuration**
    In the product mode, the configuration is filled in the json string format. 
    
    + **ollama configuration  **
    
    ![1736172723773-12d07dc9-17d7-4cc3-8092-2db1eb442de1.png](./img/5QwPnXeg23q0GNnJ/1736172723773-12d07dc9-17d7-4cc3-8092-2db1eb442de1-894735.png)
    
    
    
    ```bash
    # localhost need to be replaced with real host_addr, which can be accessied in container
    
    {
      "client_type": "ollama",
      "base_url": "http://localhost:11434/",
      "model": "qwen2.5:3b"
    }
    ```
    
    + **vllm configuration  **
    
    ![1736172866279-177daded-0b47-42ae-9bd3-d06b22839103.png](./img/5QwPnXeg23q0GNnJ/1736172866279-177daded-0b47-42ae-9bd3-d06b22839103-804076.png)
    
    
    
    ```bash
    # localhost need to be replaced with real host_addr, which can be accessied in container
    
    {
      "client_type": "vllm",
      "base_url": "http://localhost:8000/v1/chat/completions",
      "model": "qwen25-7B-chat"
    }
    ```
    
    + **other**
    
    If you prefer to use other Model Inference Service Frameworks (such as xinference), you can build them yourself. Set client_type = vllm (equivalent to the default value) before calling, and base_url = xx. 
    
    + **model Service Availability Test**
    
    When the configuration is saved, kag calls the big model api based on the generated model configuration. If the call fails, it prompts that the save fails. You can use the curl Command in the openspg container to verify the reachability of the service to avoid the impact of service calls when the model is stopped. For details, please refer to the section [Container Access Host Model Service](#mjSk7).
    
    ### 2.2.2 **Developer Mode Configuration **
    you can change the large model that all aspects of kag depend on according to the configuration file kag_config.yaml.  
    In developer mode, large model calls related to knowledge extraction and inference question and answer are initiated from the local environment where kag runs. The domain name corresponding to the model inference service, which needs to be resolved in the environment where the kag runs. 
    
    + **ollama**
    
    ```bash
    # base_url need to be replaced with real host_addr which can accessied in kag runtime。
    openie_llm: &openie_llm
      base_url: http://localhost:11434/
      model: qwen2.5:3b
      type: ollama
    
    chat_llm: &chat_llm
      base_url: http://localhost:11434/
      model: qwen2.5:3b
      type: ollama
    ```
    
    + **vllm**
    
    ```bash
    # localhost need to be replaced with real host_addr which can accessied in kag runtime。
    openie_llm: &openie_llm
      base_url: http://localhost:8000/v1/chat/completions
      model: qwen25-7B-chat
      type: ollama
    
    chat_llm: &chat_llm
      base_url: http://localhost:8000/v1/chat/completions
      model: qwen25-7B-chat
      type: ollama
    ```
    
    Note: In developer mode, after the configuration is modified, it needs to be synchronized to the server. Specific reference documents: [update Project Configuration](https://openspg.yuque.com/ndx6g9/cwh47i/rs7gr8g4s538b1n7#vmE7s).
风险细节：
    敏感类型：URL地址(url)
    敏感细节：{hitWord=https://www.deepseek.com/)}
{hitWord=https://chat.openai.com/)}
{hitWord=https://api.deepseek.com}
{hitWord=https://api.deepseek.com/chat/completions}
{hitWord=https://api.openai.com}
{hitWord=https://api.openai.com}
{hitWord=https://dashscope.aliyuncs.com/compatible-mode/v1}
{hitWord=https://dashscope.aliyuncs.com/compatible-mode/v1}
{hitWord=https://api.deepseek.com}
{hitWord=https://api.deepseek.com}
{hitWord=https://openspg.yuque.com/ndx6g9/cwh47i/rs7gr8g4s538b1n7#vmE7s).}
{hitWord=https://ollama.com/)download}
{hitWord=https://blog.vllm.ai/2023/06/20/vllm.html)said:}
{hitWord=https://www.lixueduan.com/posts/ai/01-how-to-use-gpu/)}
{hitWord=http://host.docker.internal:11434/v1/chat/completions}
{hitWord=https://openspg.yuque.com/ndx6g9/cwh47i/rs7gr8g4s538b1n7#vmE7s).}
风险内容位置：
    CommitId: 2911f9fb9d07e718b153121318ef12824a30a299
    文件名：website/docs/en/UserGuide/SPGReasoner/KGDSL.md
    文件行：1~1072
风险内容：
    ---
    sidebar_position: 1
    ---
    
    # KGDSL
    
    Note: KGDSL is case-insensitive.
    
    ## 1 Reserved Keywords
    ### 1.1 Common Keywords
    | kewords | description | scopes |
    | --- | --- | --- |
    | Define | keywords for define predicate | global |
    | Structure | Keywords for subgraph description | global |
    | Constraint | Keywords for Constraint description | global |
    | Action | Keywords for action | global |
    | / | Concept reference delimiter | global |
    | group | keywords for group | Constraint |
    | sum/filter/find/sort/slice   /count/max/min/avg/concat | group operators | after group in Constraint |
    | and/or/not/xor/optional | logical operator | global |
    
    
    ### 1.2 Special Keywords
    | keywords | description | scopes |
    | --- | --- | --- |
    | **start** | Start symbol | Structure |
    | **per_node_limit** | limit symbol | Structure |
    | **label** | type of entity or relation | Constraint/Action |
    | **property_map** | mapping object for property | Constraint/Action |
    | **path** | Obtain paths that satisfy the conditions | Constraint/Action |
    | **id** | unique id of the entity | Constraint/Action |
    | **from** | source entity id of the relation | Constraint/Action |
    | **to** | destination entity id of the relation | Constraint/Action |
    
    
    ## 2 Data Type
    ### 2.1 Basic Data Type
    | Data Type | description | example |
    | --- | --- | --- |
    | int | integer number | 1，2，3 |
    | float | float number | 23.11 |
    | string | string | "abcdef" |
    | bool | boolean | true/false |
    | null | null | null |
    
    
    ### 2.2 Complex Data Type
    | Data Type | description | example |
    | --- | --- | --- |
    | list | array type | [1,2,3] |
    | multi_version | multi version property | `{ "20220111": value, "20220112": value }` |
    | date | date type | / |
    | node | entity type | `{   "id":123456,   "label":"Film",   "property":{"name":"Titanic"}   }` |
    | edge | relation type | `{   "from":1234,   "to":4321,   "label":"starOfFilm",   "property":{"year":1989}   }` |
    
    
    ## 3 Expression Operators
    ### 3.1 Expression format
    We use a combination of procedural and chaining expressions in our code. 
    
    > Chaining approach: It links multiple operations or lines of code together using dot notation (.) to improve code readability.   
    Procedural approach: It describes a computation using multiple lines of code. 
    >
    
    The chaining style is particularly suitable for data calculations. For example, consider the expression (1 + 2) * 3 - 4.  
    If we need to compute it step by step, the procedural approach would be: 
    
    > a = 1+2   
      
    b = a *3   
      
    d = b -4 
    >
    
    the chaining approach would be:
    
    > add(1,2).multiply(3).subtract(4) 
    >
    
    using the chaining style allows us to express a complete computational flow in a single line, making it particularly useful for data calculations. It allows for concise and readable code, where each method call builds upon the previous one to perform a series of operations. This style can enhance code clarity and simplify complex calculations, especially when dealing with data transformations, aggregations, or other data processing tasks.
    
    ### 3.2 Compute Operators
    | Operator | example | description | note |
    | --- | --- | --- | --- |
    | + | a+b | addition | |
    | - | a-b | subtraction | |
    | * | a*b | Multiplication | |
    | / | a/b | division | Non divisible 0 |
    | % | a%b | modulus | b can't be 0 |
    | = | a=b | Assignment | |
    
    
    ### 3.3 Logical Operators
    | Operator | example | description | note |
    | --- | --- | --- | --- |
    | and | a and b | and operation | |
    | or | a or b | or operation | |
    | not，! | not a, !a | not operation | Not can be applied globally, but ! Can only apply in Constraint |
    | xor | a xor b | XOR operation | |
    | optional | optional (a)-[e]->(b) | optional path | optional can be applied in Structure |
    
    
    ### 3.4 Compare Opeators
    | Operator | example | description | note |
    | --- | --- | --- | --- |
    | == | a == b | equal | can be applied to int/float/string/node/edge,   specailly,node/edge will compared with id |
    | > | a > b | bigger | |
    | >= | a>=b | bigger or equal | |
    | `<` | a < b | smaller | |
    | `<=` | `a<=b` | smaller or equal | |
    | != | a != b | not equal | |
    | in | a in [1,2,3] | contain | |
    | BT | a bt [1,5]     a bt (1,5) | between operator，a is between 1 and 5 | can be applied to int/float/string |
    
    
    ### 3.5 String Operators
    | Operator | example | description | return value | note |
    | --- | --- | --- | --- | --- |
    | contains | contains(a,b) | Determine whether the a string contains the b string | bool | |
    | like, not like | a like b, a not like b | String matching, % is a wildcard character | bool | "abc" like "a%" = true |
    | concat, + | concat(a,b), a+b,     concat(a,b,c), a+b+c     concat(a,...,f), a+...+f | String concatenation, concat supports n input parameters,   can also be used + opeator | string | not implemented |
    | length | length(a) | return length of string | int | |
    | strstr | strstr(str,start)     strstr(str,start,end) | return sub string of str | string | not implemented |
    | lower | lower(a) | Convert to lowercase | string | not implemented |
    | upper | upper(a) | Convert to uppercase | string | not implemented |
    | is_not_blank | is_not_blank(a) | the string is not empty | bool | not implemented |
    
    
    ### 3.6 Type Conversion Operators
    | Operator | example | description | note |
    | --- | --- | --- | --- |
    | cast(a, 'int'/'float'/'string') |  cast(1,'string')     cast('1', 'int') | Convert a base type to another base type | |
    | to_date(time_str,format) | to_date('20220101', 'yyMMdd') | Convert a string to a date type   format can be a combination of   time types     - s: Seconds//Unix timestamp    - m: minutes     - h: hours     - d: day     - M: month     - y: year     Can be combined with various reasonable formats     - yyMMdd     - yyMMdd hh:mm:ss | not implemented |
    | window(version_express, unit) | `A.cost_every_day     A.cost_every_day.window(cur in [1@M,2@M,3@M], M)     A.cost_every_day.window(start > -30@d, d)     A.cost_every_day.window(end <-15@d, d)     A.cost_every_day.window(start > -30@d and end <-15@d, d)     A.cost_every_day.window((start > -30@d and end <-15@d) and (start > -7@d), d)` | Convert a multi-version attribute to a list for calculation purposes. The **version_express** consists of three keywords:     - start: Starting version number     - end: Ending version number     - cur: Current version number. Expressions can be combined using logical operators like AND/OR.   **unit** The unit parameter defines the unit of the attribute, with options:     - M: Retrieve data by month     - d: Retrieve data by day     - seq: Default value, retrieves data by sequence if no unit is specified.     Note: If retrieving data by month or day, the data must be aggregated by day or month. | Not implemented |
    
    
    ### 3.7 List Operators (Not Implemented)
    Since lists can support different types such as int, float, string, node, and edge, the list operators are categorized based on these types. For list objects, we use a chaining style to perform calculations on lists.
    
    Assuming we define an array as:
    
    ```plain
    array = [{age:10},{age:20},{age:30}]
    ```
    
    the operations on this array are as follows:
    
    | Operator | Example | Description | Input type | Output type | Element Type |
    | --- | --- | --- | --- | --- | --- |
    | max(alias_name) | array.mark_alias(a).max(a.age)    //return 30 | Get the maximum value | list | int/float/string | int,float,string |
    | min(alias_name) | array.mark_alias(a).min(a.age)    //return 10 | Get the minimum value | list | int/float/string | int,float,string |
    | sum(alias_name) | array.mark_alias(a).sum(a.age)    // return 60 | Accumulate the values in the list | list | int/float | int,float,string |
    | avg(alias_name) | array.mark_alias(a).avg(a.age)    // return 20 | Get the average value | list | int/float | int,float,string |
    | count() | array.count()    //return 3 | Get the size of the list | list | int | All types |
    | filter(operator_express) | `array.mark_alias(a).filter(a.age <18)    //return [{age:10}]` | Filter the list based on the given expression | list | list | All types |
    | sort(alias_name, 'desc'/'asc') | `array.mark_alias(a).sort(a.age, 'desc')    //return [{age:30},{age:20},{age:10}]` | Sort the list in ascending or descending order | list | list | All types |
    | slice(start_index,end_index) | `array.mark_alias(a).slice(1,2)    //return [{age:10},{age:20}]` | Get a slice of the list from the start index to the end index | | | All types |
    | get(index) | `array.mark_alias(a).get(1)    //return {age:10}` | Get the element at the specified index | | | |
    | str_join(alias_name, tok) | array.mark_alias(a).str_join(cast(a.age, 'string'), ',')    //return "10,20,30" | Join the elements of the list into a string using a delimiter | | | string |
    | accumlate(operator, alias_name) | array.mark_alias(a).accumlate('*', a.age) //return 6000    array.mark_alias(a).accumlate('+', a.age) //return 60 | Perform cumulative calculation on the list using the operator | | | int,float |
    
    
    ### 3.8 Graph Aggregation Operators
    Since there are often aggregation calculations on graphs, we define a graph aggregation operator that can aggregate a subgraph according to a specified pattern and perform array calculations based on aliases.
    
    | operator | Example | Description | Input type | Output type | Note |
    | --- | --- | --- | --- | --- | --- |
    | group() | group(a)，group(a,b) | Aggregate vertices or edges and return an array | Graph | Array | Input must be vertices or edges, and the starting point must be included |
    
    
    Explanation of graph grouping:   
    Assuming we have the following data:   
    ![original](./img/v9u6-LTYH1EP0EOx/1735477071854-4b3ea4e8-69b7-46b3-b441-49ec5f4ce6d1-399550.png)  
      
    The query subgraph pattern is:  
    ![original](./img/v9u6-LTYH1EP0EOx/1735477071733-0c347ffe-5371-4885-916a-de1c2641d134-317374.png)
    
    group operator examples: 
    
    #### 3.8.1 example 1：group(A) 
    The grouping of A in this case would result in the following operations: 
    
    > The return type is a list. Since the entire subgraph is grouped into one, the length of the returned list would be 1, and the subsequent results can only output one row of data.   
    group(A).count(e1) // Count the edges of type e1, should return [2]   
    group(A).count(B) // Count the occurrences of type B in the subgraph, should return [2]   
    group(A).count(C) // Count the occurrences of type C in the subgraph, should return [4]   
    group(A).count(e2) // Count the edges of type e2, should return [5] because there are 5 edges 
    >
    
    #### 3.8.2 example 2：group(A,B) 
    Suppose we have graph datas:   
      
    ![original](./img/v9u6-LTYH1EP0EOx/1736735540999-11d747c9-c181-4b92-bee6-c7b0ba063d1f-346809.png) 
    
    > The return type is a list. Since the entire subgraph is grouped into two, the length of the returned list would be 2,  
    and the subsequent results can only output two rows of data.   
    group(A,B).count(A) // return [1,1]   
    group(A,B),count(B) // return [1,1]   
    group(A,B).count(C) // return [3,1]   
    group(A,B).count(e1) // return [1,1]   
    group(A,B).count(e2) // return [3,2] 
    >
    
    #### 3.8.3 example 3：group(A,B,C) 
    Suppose we have graph datas:   
      
    ![original](./img/v9u6-LTYH1EP0EOx/1735477071750-72fd9f39-f059-420f-8876-161c936dd618-686613.png) 
    
    > The return type is a list. Since the entire subgraph is grouped into four, the length of the returned list would be 4,  
    and the subsequent results can only output four rows of data.   
    group(A,B,C).count(C) // return [1,1,1,1]   
    group(A,B,C).count(e2) // return [1,1,1,2] 
    >
    
    Note: Since the subgraph may be divided into multiple subgraphs, the order of the returned array is not guaranteed.
    
    #### 3.8.4 Constraints and Limitations
    #### Constraint 1: Not allowing to exclude the starting point
    + group(B)/group(C), It is not allowed to group without including the starting point, as this grouping cannot guarantee correctness.
    
    #### Constraint 2: Not allowing grouping by edges
    + group(A, e1)/group(A, e2), It is not allowed to group by edges, as there may be multiple edges between the same vertices. Allowing grouping by edges would result in a large number of duplicate nodes, leading to a significant increase in computational cost. Additionally, there hasn't been seen a scenario where grouping by edges is necessary.
    
    #### Constraint 3: In case of using multiple groups, the later group should not have more vertices than the previous group
    ```plain
    bNum = group(A).count(B)
    eNum = group(A,B).count(e1)
    ```
    
    ```plain
    eNum = group(A,B).count(e1)
    bNum = group(A).count(B)
    ```
    
    The reason is that when group(A) is performed, the vertices of B are folded. In this case, grouping A and B will result in incorrect results. This constraint is mainly considered for implementation factors.
    
    ### 3.9 Data Retrieval Operators
    To facilitate data retrieval from graphs, the following operators are defined:
    
    | Operator | Example | Description | Note |
    | --- | --- | --- | --- |
    | . | A.id | Retrieve attribute | |
    | **label** | A.**label** | Return type | |
    | **from** | e.**from** | Return the starting point ID | |
    | **to** | e.**to** | Return the ending point ID | |
    | **direction** | e.**direction** | Retrieve the direction of the edge | |
    
    
    Since KGDSL 2.0 has removed the if syntax, we need to use conditional operator operators to replace the logical conditions.   
    **rule_value** 
    
    + Syntax: rule_value(rule_name, true_value, false_value)
    + Description: Converts the truth value of the rule_name to the specified value. If the result of the rule_name is true, it returns true_value; if it is false, it returns false_value.
    
    Example:
    
    ```plain
    //if the result of the rule "OnlineDiscount" is true, it returns 1, otherwise it returns null.
    rule_value("OnlineDiscount", 1, null)
    ```
    
    
    
    **get_first_notnull** 
    
    + Syntax: get_first_notnull(value1, value2, ..., valueN)
    + Description: Returns the first non-null value from the arguments. The parameter list is variable-length, allowing for prioritized retrieval of values.
    
    Example:
    
    ```plain
    Share10("Share more than 10 orders"): rakeBackCount > 10
    Share100("Share more than 100 orders"): rakeBackCount > 100
    Price("Pricing")= get_first_notnull(rule_value("Share100", 0.5, null), rule_value("Share10", 0.8, null))
    ```
    
    With the combination of these two UDFs, arbitrary if-else combinations can be achieved.
    
    ### 3.10 Date Operators (Not Implemented)
    The date type supports the following calculation operations:
    
    | Operator | Example | Description | Input type | Output type |
    | --- | --- | --- | --- | --- |
    | + | date1 = to_date('20220212', 'yyMMdd')     date2 = to_date('5','d')     date3 = date1 + date2 //return 20220217 | Add date | date | date |
    | - | date1 = to_date('20220212', 'yyMMdd')    date2 = to_date('5','d')    date3 = date1 - date2 //return 20220207 | Subtract date | date | date |
    
    
    #### 3.10.1 Simplified Date Initialization
    To simplify the description and avoid the need for explicit to_date conversions for date types, the following format can be used for date initialization: 
    
    > Date/Unit 
    >
    
    **Example 1: Simplified date initialization mode** 
    
    ```sql
    1@d
    1@h
    1@M
    20221011@yyMMdd
    ```
    
    **Example 2: Relative date based on the current time**   
    To address the common requirement of expressing "last 30 days," "last 7 days," etc., the simplified form now() can be used to represent the current time. Examples include:
    
    ```sql
    +1@d
    -1@d
    +1@M
    ```
    
    In addition to these simplified date expressions, there are also other date functions available.
    
    #### 3.10.2 now
    + Syntax: now()
    + Description: A date calculation function that returns the current date.
    
    Example:
    
    ```plain
    now()
    ```
    
    #### 3.10.3 date_format
    + Syntax: date_format(time, to_format) / date_format(time, from_format, to_format)
    + Description: A date formatting function that converts a date to a specified string format. The default format is yyyy-MM-dd HH:mm:ss or yyyyMMdd HH:mm:ss.
    
    Example:
    
    ```plain
    date1 = to_date('20220101', 'yyMMdd')
    date_format(date1, 's') // Converts to a Unix timestamp, with a value of 1640966400
    date_format(date1, 'yyMMdd hh:mm:ss') // Converts to the specified format, should be "20220101 00:00:00"
    ```
    
    ## 4 Basic Syntax
    In this section, we will introduce and apply the syntax based on specific use cases.
    
    ### 4.1 Example Scenarios and Requirements
    #### 4.1.1 Example Schema
    Assuming the schema is as follows:   
    ![original](./img/v9u6-LTYH1EP0EOx/1735477071708-18e30ecc-236c-4dce-99da-65da6288f07d-328027.png) 
    
    **User Entity**
    
    | Property Name | Type | Description |
    | --- | --- | --- |
    | id | string | Primary key ID |
    | name | string | user name |
    | age | int | age |
    | gender | Gender concept | Gender concept |
    
    
    **Shop Entity**
    
    | Property Name | Type | Description |
    | --- | --- | --- |
    | id | string | Primary key ID |
    | name | string | shop name |
    | category | Classification Concepts | Classification for shops |
    
    
    **(User)-[pay]->(User) Relation**
    
    the relation which represents the payment between users.
    
    | Property Name | Type | Description |
    | --- | --- | --- |
    | amount | float | The amount of the payment. |
    
    
    **(User)-[visit]->(Shop) Relation**
    
    the relation which represents the user browsed a certain store
    
    | Property Name | Type | Description |
    | --- | --- | --- |
    | timestamp | int | The timestamp of the browse. |
    
    
    **(User)-[own]->(Shop) Relation**
    
    The relation which represents the user owned a certain store.   
    Has no properties.
    
    **(User)-[consume]->(Shop) Relation**
    
    The relation which represents the user has made a purchase at a certain store.
    
    | Property Name | Type | Description |
    | --- | --- | --- |
    | amount | float | The amount of the purchase. |
    | timestamp | int | The timestamp of the purchase. |
    
    
    #### 4.1.2 Requirements
    | Requirement ID | Description |
    | --- | --- |
    | 1 | Determine if a User is a shop owner |
    | 3 | Calculate the number of times a Shop has been browsed in the last 7 and 30 days |
    | 4 | Classify Shops into high attention and low attention based on the number of times they were browsed in the last 7 days |
    | 5 | Identify the top 3 Users with the highest spending based on the sales of a Shop in the last 7 days |
    | 6 | Validate if a user has received more money in transfers than they have spent |
    | 7 | Check if a user has made a transfer to themselvesxa |
    | 8 | Get the list of other users that a User has transferred money to in the last 7 days |
    | 9 | Users own their own shops and make purchases in their own shops |
    | 10 | Count the number of shops a User has made purchases or browsed in the last 7 days |
    | 11 | Calculate the total expenditure of each user at a specific shop |
    
    
    ### 4.2 Overall syntax description
    Logic rules are represented in a three-part syntax, syntax structures are as follows:
    
    ```plain
    #The Structure section defines the matched subgraph structure using a path description. It specifies the entities and relationships that need to be present in the subgraph.
    Structure {
        // path desciption
    }
    #The Constraint section defines the constraints on entities and relationships within the Structure. It specifies the conditions that must be satisfied for a subgraph to be considered a valid match. It can also include expressions for rule calculations.
    Constraint {
        // rule express
    }
    #The Action section specifies the post-processing to be performed on the results that conform to the Structure and Constraint.
    Action {
        // action desciption
    }
    ```
    
    The syntax structure for defining new logical predicates is as follows:
    
    ```plain
    #Structure and Constraint are nested inside the Define section.
    #The Define section is used to define a new logical predicate. It allows you to create a custom predicate with its own Structure and Constraint.
    Define (s:sType)-[p:pType]->(o:oType) {
        Structure {
            // path desciption
        }
        Constraint {
            // rule express
        }
    }
    ```
    
    
    
    In the following sections, we will provide a detailed explanation of the usage of Structure, Constraint,Action, and Define.
    
    ### 4.3 Structure
    This section primarily describes path structures.
    
    #### 4.3.1 Path Definition
    The basic unit of a path is an edge. A path is formed by combining multiple edges in a connected graph. The Structure can describe multiple paths, making it convenient for use in different scenarios. Path descriptions can be done using the ISO GQL (Graph Query Language) format. Here are three examples:
    
    ```plain
    Structure {
        (s:User)-[p:own]->(o:Shop)
    }
    ```
    
    
    
    ```plain
    Structure {
        (s:User)-[p:own]->(o:Shop), (s)-[c:consume]->(o)
    }
    ```
    
    > Note: The both edges must exist, indicated by a comma. 
    >
    
    ```plain
    Structure {
        (s:User)-[p:own|consume]->(o:Shop)
    }
    ```
    
    #### 4.3.2 Path Alias
    The main purpose of Structure is to simplify path description. In most scenarios, we need to determine the existence of a path for subsequent rule calculations. To facilitate this, we use path alias as the parameter for judging the existence of a path in the Constraint, as follows.
    
    ```plain
    Structure {
        path: (s:User)-[p:own]->(o:Shop)
    }
    ```
    
    > When the user "s" owns a store, the path is true; otherwise, it is false. 
    >
    
    ```plain
    Structure {
        path: (s:User)-[p:own]->(o:Shop), (s)-[c:consume]->(o)
    }
    ```
    
    > When the user "s" owns a store and has made a purchase in that store, the path is true; otherwise, it is false. 
    >
    
    ```plain
    Structure {
        not path: (s:User)-[p:own|consume]->(o:Shop)
    }
    ```
    
    > When the user "s" has not made any purchases in any store and does not own any store, the path is false; otherwise, it is true. 
    >
    
    The advantage of using aliases is that it simplifies the description of path paths. The above two can be described as follows:
    
    ```plain
    Structure {
        ownPath: (s:User)-[p:own]->(o:Shop)
        consumePath: (s)-[c:consume]->(o)
    }
    ```
    
    Declare two paths: 
    
    + "The user owns their own store and has made a purchase in their own store" can be expressed as "ownPath and consumePath".
    + "The user owns their own store or has made a purchase in any store" can be expressed as "ownPath or consumePath".
    
    #### 4.3.3 Path Operators
    In path expressions, it is possible to specify that the path in Structure is not mandatory. ISO GQL's path expressions already include the expressions for "and," "or," "optional," and "not." We will maintain consistency with ISO GQL in this regard.
    
    ```plain
    Structure {
        path: (s:User)-[p:own]->(o:Shop), (s)-[c:consume]->(o)
    }
    ```
    
    ```plain
    Structure {
        path: (s:User)-[p:own|consume]->(o:Shop)
    }
    ```
    
    （Not implemented.）
    
    ```plain
    Structure {
        not path: (s:User)-[p:own]->(o:Shop)
    }
    ```
    
    （Not implemented.）
    
    ```plain
    Structure {
        optional path:(s:User)-[p:own]->(o:Shop), (s)-[c:consume]->(o)
    }
    ```
    
    ### 4.4 Constraint
    #### 4.4.1 Single Rule Syntax
    In the Constraint statement, each line represents a separate rule, which can be classified into the following  
    categories:
    
    +  **Logical Rule**  
    Expressed as:  **RuleName("Rule description"): expression**   
    The output of a logical rule is a boolean value. Commonly used operators include `>, <, ==, >=, <=, !=, +, -, *, /, %`, and so on. These operators can be extended as needed. 
    +  **Calculation Rule**  
    Expressed as: **RuleName("Rule description") = expression**   
    The output of a calculation rule are numbers or text, depending on the content of the expression. 
    +  **Assignment Rule**  
    Expressed as: **Alias.property = expression**   
    Assignment rules are used to assign values to properties that defined in the rule. These rules are only valid within specific predicate definitions. 
    
    Using the example of ownPath and consumePath from section 4.3: 
    
    ```plain
    Structure {
        ownPath: (s:User)-[p:own]->(o:Shop)
        consumePath: (s)-[c:consume]->(o)
    }
    Constraint {
    }
    ```
    
    ```plain
    Structure {
        optional ownPath: (s:User)-[p:own]->(o:Shop)
        optional consumePath: (s)-[c:consume]->(o)
    }
    Constraint {
        ownAndConsumeUser("User owns their own store or has made purchases in any store"): exist(ownPath) or exist(consumePath)
    }
    ```
    
    ```plain
    Structure {
        (s:Shop)<-[p:visit]-(o:User)
    }
    Constraint {
        R("Whether the user has visited within the last 7 days") : p.timestamp >= -7@d
    }
    ```
    
    > If there are multiple users accessing and we only want to consider edges within the last 7 days, the rule will be terminated processing for any vertex that does not satisfy the condition in the fifth line. 
    >
    
    #### 4.4.2 Rule Group
    The rule group can combine logical rules, with the main purpose of hierarchizing logical calculations.
    
    ```plain
    Structure {
        （s:User)
    }
    Constraint {
        R1("Adult"): s.age > 18
    
        R2("Male"): s.gender == "male"
    
        // The following sentence is correct: R3, composed of R1 and R2, is considered as a rule group.
        R3("Adult male"): R1 and R2
    
        // The following sentence is incorrect: Non-rule variables are not allowed in a rule group.
        R3("Adult male"): R1 and s.gender == "male"
    }
    ```
    
    #### 4.4.3 Aggregation syntax
    Aggregation operators have the following features and limitations:
    
    + The input of the operator must be a list.
    + The group statement can be used to group the graph, aggregating multiple paths with the same pattern into groups. When using group, the aggregation operator performs aggregation operations on the nodes and edges of the grouped graph.
    + Aggregation operators can only perform aggregation calculations on subgraphs generated by a single starting entity instance. **If calculations are needed for a batch of subgraphs generated by multiple starting points, it is not supported within the scope of this document**.
    
    The list of aggregation and statistical requirements in section 4.1.2 is as follows:
    
    | Requirement ID | Description |
    | --- | --- |
    | 3 | Calculate the number of times a shop has been browsed in the past 7 days or 30 days |
    | 4 | Classify Shops into high attention and low attention based on the number of times they were browsed in the last 7 days |
    | 5 | Identify the top 3 Users with the highest spending based on the sales of a Shop in the last 7 days |
    | 6 | Validate if a user has received more money in transfers than he have spent |
    | 10 | Count the number of shops a User has made purchases or browsed in the last 7 days |
    | 11 | Calculate the total expenditure of each user at a specific shop |
    
    
    **Example 1：Requirement 10, Requirement 3 and Requirement 4**   
      
    ![original](./img/v9u6-LTYH1EP0EOx/1735477071871-901d6c90-6ad1-47d9-983f-20e25e51be83-712816.png) 
    
    Assuming the current date is January 10, 2023, the number of shops that Alice has consumed or browsed in the past 7 days should be 2. The grammar expression is as follows. 
    
    ```plain
    Structure {
        (s:User)-[p:visit|consume]->(o:Shop)
    }
    Constraint {
        R1("The shop has been consumed or browsed in the past 7 days") : p.timestamp >= -7@d
        // without group()
        visitOrConsumeShopNum("calculate the number of shops that a user has consumed or browsed in the past 7 days") = count(o)
    
        // with group
        visitOrConsumeShopNum("calculate the number of shops that a user has consumed or browsed in the past 7 days") = group(s).count(o)
    }
    ```
    
    
    
    **Example 2：Requirement 6**   
      
    ![original](./img/v9u6-LTYH1EP0EOx/1735477071822-26f1b352-b016-464e-97c9-ec958f118f04-365103.png) 
    
    In the given image, Jobs, Alice, and Mike have a surplus (income greater than expenses), while Bob has a  
    deficit (expenses greater than income). The rules can be expressed as follows. 
    
    ```plain
    Structure {
        outPath: (s:User)-[outP:pay]->(outU:User)
        inPath: (inU:User)-[inP:pay]->(s)
    }
    Constraint {
        // If inPath does not exist, return 0. Otherwise, proceed with the aggregation calculation.
        inAmount("income") = rule_value(inPath, group(s).sum(inP.amount), 0)
        // If outPath does not exist, return 0. Otherwise, proceed with the aggregation calculation.
        outAmount("expenses") = rule_value(outPath, group(s).sum(outP.amount), 0)
    
        R2("surplus"): inAmount > outAmount
    }
    ```
    
    **Example 3：Requirement 5**（**Not implemented**）** 
    
    ![original](./img/v9u6-LTYH1EP0EOx/1735477071818-e3b75d8f-bc51-4f0c-b177-c1cf5d79002b-246783.png)   
    In the given data example, the top 3 are Jobs, Mike and Alice. 
    
    ```plain
    Structure {
        (s:Shop)<-[p:consume]-(o:User)
    }
    Constraint {
        R1("Expenses within 7 days"): p.timestamp >= -7@d
        R2("top3 users"): group(s).desc(p.amount).limit(3)
    }
    ```
    
    **Example 4：Requirement 11**   
    Assuming the data is as follows: 
    
    ![original](./img/v9u6-LTYH1EP0EOx/1735477071716-7deac508-edf3-4cb1-a371-cdef6431dbdd-361056.png) 
    
    We need to calculate the number of times the shop has been browsed in the past 7 days. it is important to note that there are 2 edges for Bob, so they need to be aggregated for the statistics. 
    
    ```plain
    Structure {
        (s:Shop)<-[p:consume]-(o:User)
    }
    Constraint {
        R("Expenses within 7 days"): p.timestamp >= -7@d
        // To assign the total amount of user expenses
        userConsumeAmount("the total amount of user expenses") = group(s,o).sum(p.amount)
    }
    Action {
        get(s.name, userConsumeAmount)
    }
    ```
    
    ### 4.5 Define predicate rule
    The main purpose of the previous chapters was to describe paths and rules, while the focus of this section is to define predicates. Predicates are mainly expressed in three scenarios:
    
    + Inductive semantic definition between entity types and concepts.
    + Logical predicate definition between entity types.
    + Logical predicate definition between entity types and basic types.
    
    #### 4.5.1 Inductive semantic definition between entity types and concepts
    For information about entities and concepts, please refer to the explanation in [the schema modeling manual](https://openspg.yuque.com/ndx6g9/ps5q6b/mi62ex5rubsuc8mb). 
    
    Inductive semantics refers to the process of deriving general concepts from a group of entities with common characteristics. The relation between these individuals and concepts is known as an inductive relation.
    
    The inductive semantics between entity types and concepts can be expressed through the following syntactic rules:
    
    ```plain
    Define (s:TypeA)-[p:TypeP]->(o:TaxonomyOfTypeA/ConceptA) {
        Structure {
            // path desciption
        }
        Constraint {
            // rule express
        }
    }
    ```
    
    ConceptA belongs to the TaxonomyOfTypeA. The above rule expresses that the entity s, under the premise of satisfying the above rule expression, it can be linked to the ConceptA through the TypeP predicate. For example:   
    According to the requirement in section 4.1.2, we can transform the following requirement into a concept definition.
    
    | Requirement ID | Description |
    | --- | --- |
    | 1 | Determine if a User is a shop owner |
    | 4 | Classify Shops into high attention and low attention based on the number of times they were browsed in the last 7 days |
    | 7 | Check if a user has made a transfer to themselves |
    
    
    Example 1：Determine if a User is a shop owner   
    Assuming the concept of "ShopKeeper" has been modeled and created, as shown below:   
      
    ![original](./img/v9u6-LTYH1EP0EOx/1735477071764-a7a5ff28-0a9e-43ff-b005-a12ad6c6b9f8-796673.png) 
    
    In the instance diagram, it can be observed that Bob does not have a store and does not belong to the "ShopKeeper" category. However, Alice has a Hotel, so he should be classified as a "ShopKeeper". We can establish a relation between Alice and the "ShopKeeper" concept. 
    
    ```plain
    Define (s:User)-[p:belongTo]->(o:TaxonomyOfUser/ShopKeeper) {
        Structure {
            path: (s)-[ownP:own]->(shop:Shop)
        }
        Constraint {
            R1("own a shop or not"): path
        }
    }
    ```
    
    By applying the above rule, we can establish a relationship between concepts and entity instances.
    
    Example 2: Based on the frequency of visits to the shop in the past 7 days, classify the shops into high-attention shops and low-attention shops.   
      
    ![original](./img/v9u6-LTYH1EP0EOx/1735477071723-5ad295da-e99a-4952-9592-a4f102b4f1fd-086661.png)
    
    
    
    In the above instance diagram, the hotel has been visited frequently, while the drug store has been  
    visited very few times. According to business requirements, we need to classify them respectively as "PopularShop" and "NamelessShop". 
    
    ```plain
    Define (s:Shop)-[p:belongTo]->(o:TaxonomyOfShop/PopularShop) {
        Structure {
            path  : (s)<-[vP:visit]-(u:User)
        }
        Constraint {
            R1("Expenses within 7 days"): vP.timestamp >= -7@d
            // When the path does not exist, the browsing count is 0. Otherwise, it is counted for "u".
            visitsNum("browsing count") = rule_value(path, group(s).count(u),0)
            R2("hot shop"): visitsNum > ${hot_shop_threashold}
      }
    }
    ```
    
    > Note: ${hot_shop_threshold} is a threshold parameter that needs to be filled in with a specific value when using the predicate. 
    >
    
    ```plain
    Define (s:Shop)-[p:belongTo]->(o:TaxonomyOfShop/NamelessShop) {
        Structure {
            path: (s)<-[vP:visit]-(u:User)
        }
        Constraint {
            R1("Expenses within 7 days"): vP.timestamp >= -7@d
            // When the path does not exist, the browsing count is 0. Otherwise, it is counted for "u".
            visitsNum("browsing count") = rule_value(path, group(s).count(u),0)
            R2("NamelessShop"): visitsNum < ${nameless_shop_threashold}
        }
    }
    ```
    
    > Note: ${nameless_shop_threashold} is a threshold parameter that needs to be filled in with a specific value when using the predicate. 
    >
    
    #### 4.5.2 Logical predicate definition between entity types
    According to the requirement in section 4.1.2, we can define predicates between entity types in the following requirement.
    
    | Requirement ID | Description |
    | --- | --- |
    | 7 | Check if a user has made a transfer to themselves |
    | 8 | Get the list of other users that a User has transferred money to in the last 7 days |
    | 11 | Calculate the total expenditure of each user at a specific shop |
    
    
    The schema definitions are largely consistent with those in section 4.5.1. We need to add new schema according to the requirements.   
      
    ![original](./img/v9u6-LTYH1EP0EOx/1735477071843-1955a86c-8e04-41bc-8773-ba00f7ac4548-680736.png) 
    
    
    
    There are mainly three types of relationships. 
    
    + (s:User)-[p:transSelf]->(s) : "User performing a self-transfer"
    + (s:User)-[p:trans7Days]->(o:User) : "Users who have made transfers within the past 7 days"
    + (s:Shop)-[p:consumeAmount]->(o:User) : "Consumption amount of a particular user at a shop"
    
    Example 1: User performing a self-transfer 
    
    ```plain
    Define (s:User)-[p:transSelf]->(s) {
        Structure {
            path: (s)-[pp:pay]->(s)
        }
        Constraint {
            R1("self-transfer"): path
        }
    }
    ```
    
    Example 2: Users who have made transfers within the past 7 days 
    
    ```plain
    Define (s:User)-[p:trans7Days]->(o:User) {
        Structure {
            path: (s)-[pp:pay]->(o)
        }
        Constraint {
            R1("Expenses within 7 days"): p.timestamp > -7@d
            R2("made transfers"): path
        }
    }
    ```
    
    Example 3: Consumption amount of a particular user at a shop 
    
    ```plain
    Define (s:Shop)-[p:consumeAmount]->(o:User) {
        Structure {
            path: (s)<-[cp:consume]-(o)
        }
        Constraint {
            R1("There are consumers"): path
            p.amount = group(s,o).sum(cp.amount) //Calculate the total transaction amount
        }
    }
    ```
    
    #### 4.5.3 Logical predicate definition between entity types and basic types
    In the previous two sections, the main focus was on establishing relations between entity types and concepts. However, there are some requirements that do not interact with any other types, such as the following example.
    
    | Requirement ID | Description |
    | --- | --- |
    | 1 | Determine if a User is a shop owner |
    | 3 | Calculate the number of times a Shop has been browsed in the last 7 and 30 days |
    | 6 | Validate if a user has received more money in transfers than they have spent |
    | 7 | Check if a user has made a transfer to themselves |
    | 9 | Users own their own shops and make purchases in their own shops |
    | 10 | Count the number of shops a User has made purchases or browsed in the last 7 days |
    
    
    For the given requirement, we need to add properties to the User entity:
    
    | Property Name | type | Description |
    | --- | --- | --- |
    | isShopOwner | boolean | whether the user is a shop owner |
    | isIncomeLargeOutcome | boolean | whether the user's income is greater than their expenses |
    | 7daysVisitOrConsumeShopNum | int | Number of shops visited or consumed by the user in the past 7 days |
    
    
    We need to add properties to the Shop entity:
    
    | Property Name | type | Description |
    | --- | --- | --- |
    | 7daysVisitNum | int | Number of visitors in the past 7 days |
    | 30daysVisitNum | int | Number of visitors in the past 30 days |
    
    
    These additional properties can be defined through rules, without importing actual data.
    
    Example 1: Number of shops visited or consumed by the user in the past 7 days 
    
    ```plain
    Define (s:User)-[p:7daysVisitOrConsumeShopNum]->(o:int) {
        Structure {
            path: (s)-[vc:visit|consume]->(shop:Shop)
        }
        Constraint {
            R1("visited or consumed in the past 7 days"): p.timestamp > -7@d
            o = group(s).count(shop)   //assignment
        }
    }
    ```
    
    Example 2: Number of visitors in the past 7 days 
    
    ```plain
    Define (s:Shop)-[p:7daysVisitNum]->(o:int) {
        Structure {
            path: (s)<-[p:visit]-(u:User)
        }
        Constraint {
            R1("visited in the past 7 days"): p.timestamp > -7@d
            o = group(s).count(u)   //assignment
        }
    }
    ```
    
    Example 3: Number of visitors in the past 30 days 
    
    ```plain
    Define (s:Shop)-[p:30daysVisitNum]->(o:int) {
        Structure {
            path: (s)<-[p:visit]-(u:User)
        }
        Constraint {
            R1("visited in the past 30 days"): p.timestamp > -30@d
            o = group(s).count(u)    //assignment
        }
    }
    ```
    
    ### 4.6 Action
    Action supports multiple operations:
    
    + createNodeInstance/createEdgeInstance: Used for the semantic expression of causal logic results.
    + get: Outputs the matched results, including entities, relations, and properties.
    
    The following examples will be shown.
    
    #### 4.6.1 Causal logic semantics
    In a knowledge graph, causal relations need to be established under certain conditions. This example refers to a case from the financial knowledge graph section of the SPG whitepaper. The causal description is as shown in the following diagram:  
      
    ![original](./img/v9u6-LTYH1EP0EOx/1735477071758-1674cc04-a8e8-4bde-b6cf-ef1018c82ea4-093002.jpeg)
    
    
    
    #### 4.6.1.1 createNodeInstance
    When the conditions for causal semantics between concepts are met, createNodeInstance will create a new instance. In this example, a new event instance is created using the following syntax:
    
    ```plain
    Define (s: `ProductChain.TaxonomyOfCompanyAccident`/`周期性行业头部上市公司停产事故`)-[p: leadTo]->(o: `ProductChain.TaxonomyOfIndustryInfluence`/`成本上升`) {
        Structure {
            (s)-[:subject]->(c:ProductChain.Company)
            (c)-[:belongIndustry]->(d:ProductChain.Industry)
            (d)-[:downstream]->(down:ProductChain.Industry)
        }
        Constraint {
            // No constraint conditions have been defined here
        }
        Action {
            downEvent = createNodeInstance(
                type=ProductChain.IndustryInfluence,
                value={
                    subject=down.id
                    objectWho="Rise"
                    influenceDegree="Rise"
                    indexTag="Cost"
                }
            )
        }
    }
    ```
    
    ****
    
    **createNodeInstance Parameter Description:** 
    
    + type: Specifies the type of entity instance to create.
    + value: Represents the specific attribute values of the instance, consisting of key-value pairs. The key is the attribute name defined in the schema, and the value can be a constant or various variables from Structure and Constraint. Note: If the key does not exist in the schema or the value does not meet the schema definition, it is considered an invalid value.
    
    **Return Value:** 
    
    + Specific instance alias, which should not overlap with variables in Structure or Constraint.
    
    In this example, we are creating a new event instance named "downEvent" with the event type "ProductChain.IndustryInfluence". The subject of the event is the "down" entity from the Structure, and the attributes represent the rising cost in that industry.
    
    #### 4.6.1.2 createEdgeInstance
    We can also create a new relation using createEdgeInstance to associate the triggered event instance with the event instance that has a causal relation. The specific usage is as follows:
    
    ```plain
    Define (s: `ProductChain.TaxonomyOfCompanyAccident`/`周期性行业头部上市公司停产事故`)-[p: leadTo]->(o: `ProductChain.TaxonomyOfIndustryInfluence`/`成本上升`) {
        Structure {
            (s)-[:subject]->(c:ProductChain.Company)
            (c)-[:belongIndustry]->(d:ProductChain.Industry)
            (d)-[:downstream]->(down:ProductChain.Industry)
        }
        Constraint {
    
        }
        Action {
            downEvent = createNodeInstance(
                type=ProductChain.IndustryInfluence,
                value={
                    subject=down.id
                    objectWho="Rise"
                    influenceDegree="Rise"
                    indexTag="Cost"
                }
            )
          #To establish a 'leadTo' edge between event 's' and the newly generated 'downEvent', representing that one event instance leads to another event instance.
          createEdgeInstance(
              src=s,
              dst=downEvent,
              type=leadTo,
              value={}
          )
        }
    }
    ```
    
    
    
    **createEdgeInstance Parameter Description:**
    
    + type: Specifies the type of the edge.
    + src：Alias of the source node, which must exist in the Structure, or be an instance created through createNodeInstance in the Action.
    + dst: Alias of the destination node, which must also comply with the constraints of 'src'.
    + value: Attribute values of the edge, also in key-value pairs, can be empty.
    
    **Return Value:**
    
    + None. The main reason is that edge instances are not referenced again in Action.
    
    #### 4.6.2 get
    The purpose of the 'get' operation is to retrieve entities, relations, properties, or temporary variables from a Structure or Constraint. The specific usage is as follows:
    
    ```plain
    Structure {
        path: (s:Shop)<-[vP:visit]-(u:User)
    }
    Constraint {
        R1("Expenses within 7 days"): vP.timestamp >= -7@d
        visitsNum("visiting count") = group(s).count(u)
        R2("hot shop"): visitsNum > 1000
    }
    Action {
        // To retrieve the IDs of the shop and user nodes, and return the 'visitsNum' variable from the Constraint
        get(s.id, u.id, visitsNum)
    }
    ```
风险细节：
    敏感类型：URL地址(url)
    敏感细节：{hitWord=https://openspg.yuque.com/ndx6g9/ps5q6b/mi62ex5rubsuc8mb).}
